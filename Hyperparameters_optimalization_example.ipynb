{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optymalizacja hiperparametrów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wczytanie potrzebnych bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T20:52:40.655200Z",
     "start_time": "2019-07-17T20:52:40.647505Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy 1.14.5\n",
      "pandas 0.24.2\n",
      "seaborn 0.24.2\n",
      "xgboost 0.24.2\n",
      "sklearn 0.20.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xgboost as xgb \n",
    "\n",
    "import warnings\n",
    "import sklearn\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score # wczytanie metryk sukcesu\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.options.display.max_columns = 250\n",
    "seed = 2019\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "print('numpy ' + str(np.__version__))\n",
    "print('pandas ' + str(pd.__version__))\n",
    "print('seaborn ' + str(pd.__version__))\n",
    "print('xgboost ' + str(pd.__version__))\n",
    "print('sklearn ' + str(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wczytanie pliku z danymi. W poniższym przykładzie użyłem danych z konkursu Kaggle: <br> \n",
    "https://www.kaggle.com/c/santander-customer-transaction-prediction/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T20:52:37.224641Z",
     "start_time": "2019-07-17T20:52:19.036078Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 202)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>var_10</th>\n",
       "      <th>var_11</th>\n",
       "      <th>var_12</th>\n",
       "      <th>var_13</th>\n",
       "      <th>var_14</th>\n",
       "      <th>var_15</th>\n",
       "      <th>var_16</th>\n",
       "      <th>var_17</th>\n",
       "      <th>var_18</th>\n",
       "      <th>var_19</th>\n",
       "      <th>var_20</th>\n",
       "      <th>var_21</th>\n",
       "      <th>var_22</th>\n",
       "      <th>var_23</th>\n",
       "      <th>var_24</th>\n",
       "      <th>var_25</th>\n",
       "      <th>var_26</th>\n",
       "      <th>var_27</th>\n",
       "      <th>var_28</th>\n",
       "      <th>var_29</th>\n",
       "      <th>var_30</th>\n",
       "      <th>var_31</th>\n",
       "      <th>var_32</th>\n",
       "      <th>var_33</th>\n",
       "      <th>var_34</th>\n",
       "      <th>var_35</th>\n",
       "      <th>var_36</th>\n",
       "      <th>var_37</th>\n",
       "      <th>var_38</th>\n",
       "      <th>var_39</th>\n",
       "      <th>var_40</th>\n",
       "      <th>var_41</th>\n",
       "      <th>var_42</th>\n",
       "      <th>var_43</th>\n",
       "      <th>var_44</th>\n",
       "      <th>var_45</th>\n",
       "      <th>var_46</th>\n",
       "      <th>var_47</th>\n",
       "      <th>var_48</th>\n",
       "      <th>var_49</th>\n",
       "      <th>var_50</th>\n",
       "      <th>var_51</th>\n",
       "      <th>var_52</th>\n",
       "      <th>var_53</th>\n",
       "      <th>var_54</th>\n",
       "      <th>var_55</th>\n",
       "      <th>var_56</th>\n",
       "      <th>var_57</th>\n",
       "      <th>var_58</th>\n",
       "      <th>var_59</th>\n",
       "      <th>var_60</th>\n",
       "      <th>var_61</th>\n",
       "      <th>var_62</th>\n",
       "      <th>var_63</th>\n",
       "      <th>var_64</th>\n",
       "      <th>var_65</th>\n",
       "      <th>var_66</th>\n",
       "      <th>var_67</th>\n",
       "      <th>var_68</th>\n",
       "      <th>var_69</th>\n",
       "      <th>var_70</th>\n",
       "      <th>var_71</th>\n",
       "      <th>var_72</th>\n",
       "      <th>var_73</th>\n",
       "      <th>var_74</th>\n",
       "      <th>var_75</th>\n",
       "      <th>var_76</th>\n",
       "      <th>var_77</th>\n",
       "      <th>var_78</th>\n",
       "      <th>var_79</th>\n",
       "      <th>var_80</th>\n",
       "      <th>var_81</th>\n",
       "      <th>var_82</th>\n",
       "      <th>var_83</th>\n",
       "      <th>var_84</th>\n",
       "      <th>var_85</th>\n",
       "      <th>var_86</th>\n",
       "      <th>var_87</th>\n",
       "      <th>var_88</th>\n",
       "      <th>var_89</th>\n",
       "      <th>var_90</th>\n",
       "      <th>var_91</th>\n",
       "      <th>var_92</th>\n",
       "      <th>var_93</th>\n",
       "      <th>var_94</th>\n",
       "      <th>var_95</th>\n",
       "      <th>var_96</th>\n",
       "      <th>var_97</th>\n",
       "      <th>var_98</th>\n",
       "      <th>var_99</th>\n",
       "      <th>var_100</th>\n",
       "      <th>var_101</th>\n",
       "      <th>var_102</th>\n",
       "      <th>var_103</th>\n",
       "      <th>var_104</th>\n",
       "      <th>var_105</th>\n",
       "      <th>var_106</th>\n",
       "      <th>var_107</th>\n",
       "      <th>var_108</th>\n",
       "      <th>var_109</th>\n",
       "      <th>var_110</th>\n",
       "      <th>var_111</th>\n",
       "      <th>var_112</th>\n",
       "      <th>var_113</th>\n",
       "      <th>var_114</th>\n",
       "      <th>var_115</th>\n",
       "      <th>var_116</th>\n",
       "      <th>var_117</th>\n",
       "      <th>var_118</th>\n",
       "      <th>var_119</th>\n",
       "      <th>var_120</th>\n",
       "      <th>var_121</th>\n",
       "      <th>var_122</th>\n",
       "      <th>var_123</th>\n",
       "      <th>var_124</th>\n",
       "      <th>var_125</th>\n",
       "      <th>var_126</th>\n",
       "      <th>var_127</th>\n",
       "      <th>var_128</th>\n",
       "      <th>var_129</th>\n",
       "      <th>var_130</th>\n",
       "      <th>var_131</th>\n",
       "      <th>var_132</th>\n",
       "      <th>var_133</th>\n",
       "      <th>var_134</th>\n",
       "      <th>var_135</th>\n",
       "      <th>var_136</th>\n",
       "      <th>var_137</th>\n",
       "      <th>var_138</th>\n",
       "      <th>var_139</th>\n",
       "      <th>var_140</th>\n",
       "      <th>var_141</th>\n",
       "      <th>var_142</th>\n",
       "      <th>var_143</th>\n",
       "      <th>var_144</th>\n",
       "      <th>var_145</th>\n",
       "      <th>var_146</th>\n",
       "      <th>var_147</th>\n",
       "      <th>var_148</th>\n",
       "      <th>var_149</th>\n",
       "      <th>var_150</th>\n",
       "      <th>var_151</th>\n",
       "      <th>var_152</th>\n",
       "      <th>var_153</th>\n",
       "      <th>var_154</th>\n",
       "      <th>var_155</th>\n",
       "      <th>var_156</th>\n",
       "      <th>var_157</th>\n",
       "      <th>var_158</th>\n",
       "      <th>var_159</th>\n",
       "      <th>var_160</th>\n",
       "      <th>var_161</th>\n",
       "      <th>var_162</th>\n",
       "      <th>var_163</th>\n",
       "      <th>var_164</th>\n",
       "      <th>var_165</th>\n",
       "      <th>var_166</th>\n",
       "      <th>var_167</th>\n",
       "      <th>var_168</th>\n",
       "      <th>var_169</th>\n",
       "      <th>var_170</th>\n",
       "      <th>var_171</th>\n",
       "      <th>var_172</th>\n",
       "      <th>var_173</th>\n",
       "      <th>var_174</th>\n",
       "      <th>var_175</th>\n",
       "      <th>var_176</th>\n",
       "      <th>var_177</th>\n",
       "      <th>var_178</th>\n",
       "      <th>var_179</th>\n",
       "      <th>var_180</th>\n",
       "      <th>var_181</th>\n",
       "      <th>var_182</th>\n",
       "      <th>var_183</th>\n",
       "      <th>var_184</th>\n",
       "      <th>var_185</th>\n",
       "      <th>var_186</th>\n",
       "      <th>var_187</th>\n",
       "      <th>var_188</th>\n",
       "      <th>var_189</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.9255</td>\n",
       "      <td>-6.7863</td>\n",
       "      <td>11.9081</td>\n",
       "      <td>5.0930</td>\n",
       "      <td>11.4607</td>\n",
       "      <td>-9.2834</td>\n",
       "      <td>5.1187</td>\n",
       "      <td>18.6266</td>\n",
       "      <td>-4.9200</td>\n",
       "      <td>5.7470</td>\n",
       "      <td>2.9252</td>\n",
       "      <td>3.1821</td>\n",
       "      <td>14.0137</td>\n",
       "      <td>0.5745</td>\n",
       "      <td>8.7989</td>\n",
       "      <td>14.5691</td>\n",
       "      <td>5.7487</td>\n",
       "      <td>-7.2393</td>\n",
       "      <td>4.2840</td>\n",
       "      <td>30.7133</td>\n",
       "      <td>10.5350</td>\n",
       "      <td>16.2191</td>\n",
       "      <td>2.5791</td>\n",
       "      <td>2.4716</td>\n",
       "      <td>14.3831</td>\n",
       "      <td>13.4325</td>\n",
       "      <td>-5.1488</td>\n",
       "      <td>-0.4073</td>\n",
       "      <td>4.9306</td>\n",
       "      <td>5.9965</td>\n",
       "      <td>-0.3085</td>\n",
       "      <td>12.9041</td>\n",
       "      <td>-3.8766</td>\n",
       "      <td>16.8911</td>\n",
       "      <td>11.1920</td>\n",
       "      <td>10.5785</td>\n",
       "      <td>0.6764</td>\n",
       "      <td>7.8871</td>\n",
       "      <td>4.6667</td>\n",
       "      <td>3.8743</td>\n",
       "      <td>-5.2387</td>\n",
       "      <td>7.3746</td>\n",
       "      <td>11.5767</td>\n",
       "      <td>12.0446</td>\n",
       "      <td>11.6418</td>\n",
       "      <td>-7.0170</td>\n",
       "      <td>5.9226</td>\n",
       "      <td>-14.2136</td>\n",
       "      <td>16.0283</td>\n",
       "      <td>5.3253</td>\n",
       "      <td>12.9194</td>\n",
       "      <td>29.0460</td>\n",
       "      <td>-0.6940</td>\n",
       "      <td>5.1736</td>\n",
       "      <td>-0.7474</td>\n",
       "      <td>14.8322</td>\n",
       "      <td>11.2668</td>\n",
       "      <td>5.3822</td>\n",
       "      <td>2.0183</td>\n",
       "      <td>10.1166</td>\n",
       "      <td>16.1828</td>\n",
       "      <td>4.9590</td>\n",
       "      <td>2.0771</td>\n",
       "      <td>-0.2154</td>\n",
       "      <td>8.6748</td>\n",
       "      <td>9.5319</td>\n",
       "      <td>5.8056</td>\n",
       "      <td>22.4321</td>\n",
       "      <td>5.0109</td>\n",
       "      <td>-4.7010</td>\n",
       "      <td>21.6374</td>\n",
       "      <td>0.5663</td>\n",
       "      <td>5.1999</td>\n",
       "      <td>8.8600</td>\n",
       "      <td>43.1127</td>\n",
       "      <td>18.3816</td>\n",
       "      <td>-2.3440</td>\n",
       "      <td>23.4104</td>\n",
       "      <td>6.5199</td>\n",
       "      <td>12.1983</td>\n",
       "      <td>13.6468</td>\n",
       "      <td>13.8372</td>\n",
       "      <td>1.3675</td>\n",
       "      <td>2.9423</td>\n",
       "      <td>-4.5213</td>\n",
       "      <td>21.4669</td>\n",
       "      <td>9.3225</td>\n",
       "      <td>16.4597</td>\n",
       "      <td>7.9984</td>\n",
       "      <td>-1.7069</td>\n",
       "      <td>-21.4494</td>\n",
       "      <td>6.7806</td>\n",
       "      <td>11.0924</td>\n",
       "      <td>9.9913</td>\n",
       "      <td>14.8421</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>8.9642</td>\n",
       "      <td>16.2572</td>\n",
       "      <td>2.1743</td>\n",
       "      <td>-3.4132</td>\n",
       "      <td>9.4763</td>\n",
       "      <td>13.3102</td>\n",
       "      <td>26.5376</td>\n",
       "      <td>1.4403</td>\n",
       "      <td>14.7100</td>\n",
       "      <td>6.0454</td>\n",
       "      <td>9.5426</td>\n",
       "      <td>17.1554</td>\n",
       "      <td>14.1104</td>\n",
       "      <td>24.3627</td>\n",
       "      <td>2.0323</td>\n",
       "      <td>6.7602</td>\n",
       "      <td>3.9141</td>\n",
       "      <td>-0.4851</td>\n",
       "      <td>2.5240</td>\n",
       "      <td>1.5093</td>\n",
       "      <td>2.5516</td>\n",
       "      <td>15.5752</td>\n",
       "      <td>-13.4221</td>\n",
       "      <td>7.2739</td>\n",
       "      <td>16.0094</td>\n",
       "      <td>9.7268</td>\n",
       "      <td>0.8897</td>\n",
       "      <td>0.7754</td>\n",
       "      <td>4.2218</td>\n",
       "      <td>12.0039</td>\n",
       "      <td>13.8571</td>\n",
       "      <td>-0.7338</td>\n",
       "      <td>-1.9245</td>\n",
       "      <td>15.4462</td>\n",
       "      <td>12.8287</td>\n",
       "      <td>0.3587</td>\n",
       "      <td>9.6508</td>\n",
       "      <td>6.5674</td>\n",
       "      <td>5.1726</td>\n",
       "      <td>3.1345</td>\n",
       "      <td>29.4547</td>\n",
       "      <td>31.4045</td>\n",
       "      <td>2.8279</td>\n",
       "      <td>15.6599</td>\n",
       "      <td>8.3307</td>\n",
       "      <td>-5.6011</td>\n",
       "      <td>19.0614</td>\n",
       "      <td>11.2663</td>\n",
       "      <td>8.6989</td>\n",
       "      <td>8.3694</td>\n",
       "      <td>11.5659</td>\n",
       "      <td>-16.4727</td>\n",
       "      <td>4.0288</td>\n",
       "      <td>17.9244</td>\n",
       "      <td>18.5177</td>\n",
       "      <td>10.7800</td>\n",
       "      <td>9.0056</td>\n",
       "      <td>16.6964</td>\n",
       "      <td>10.4838</td>\n",
       "      <td>1.6573</td>\n",
       "      <td>12.1749</td>\n",
       "      <td>-13.1324</td>\n",
       "      <td>17.6054</td>\n",
       "      <td>11.5423</td>\n",
       "      <td>15.4576</td>\n",
       "      <td>5.3133</td>\n",
       "      <td>3.6159</td>\n",
       "      <td>5.0384</td>\n",
       "      <td>6.6760</td>\n",
       "      <td>12.6644</td>\n",
       "      <td>2.7004</td>\n",
       "      <td>-0.6975</td>\n",
       "      <td>9.5981</td>\n",
       "      <td>5.4879</td>\n",
       "      <td>-4.7645</td>\n",
       "      <td>-8.4254</td>\n",
       "      <td>20.8773</td>\n",
       "      <td>3.1531</td>\n",
       "      <td>18.5618</td>\n",
       "      <td>7.7423</td>\n",
       "      <td>-10.1245</td>\n",
       "      <td>13.7241</td>\n",
       "      <td>-3.5189</td>\n",
       "      <td>1.7202</td>\n",
       "      <td>-8.4051</td>\n",
       "      <td>9.0164</td>\n",
       "      <td>3.0657</td>\n",
       "      <td>14.3691</td>\n",
       "      <td>25.8398</td>\n",
       "      <td>5.8764</td>\n",
       "      <td>11.8411</td>\n",
       "      <td>-19.7159</td>\n",
       "      <td>17.5743</td>\n",
       "      <td>0.5857</td>\n",
       "      <td>4.4354</td>\n",
       "      <td>3.9642</td>\n",
       "      <td>3.1364</td>\n",
       "      <td>1.6910</td>\n",
       "      <td>18.5227</td>\n",
       "      <td>-2.3978</td>\n",
       "      <td>7.8784</td>\n",
       "      <td>8.5635</td>\n",
       "      <td>12.7803</td>\n",
       "      <td>-1.0914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.5006</td>\n",
       "      <td>-4.1473</td>\n",
       "      <td>13.8588</td>\n",
       "      <td>5.3890</td>\n",
       "      <td>12.3622</td>\n",
       "      <td>7.0433</td>\n",
       "      <td>5.6208</td>\n",
       "      <td>16.5338</td>\n",
       "      <td>3.1468</td>\n",
       "      <td>8.0851</td>\n",
       "      <td>-0.4032</td>\n",
       "      <td>8.0585</td>\n",
       "      <td>14.0239</td>\n",
       "      <td>8.4135</td>\n",
       "      <td>5.4345</td>\n",
       "      <td>13.7003</td>\n",
       "      <td>13.8275</td>\n",
       "      <td>-15.5849</td>\n",
       "      <td>7.8000</td>\n",
       "      <td>28.5708</td>\n",
       "      <td>3.4287</td>\n",
       "      <td>2.7407</td>\n",
       "      <td>8.5524</td>\n",
       "      <td>3.3716</td>\n",
       "      <td>6.9779</td>\n",
       "      <td>13.8910</td>\n",
       "      <td>-11.7684</td>\n",
       "      <td>-2.5586</td>\n",
       "      <td>5.0464</td>\n",
       "      <td>0.5481</td>\n",
       "      <td>-9.2987</td>\n",
       "      <td>7.8755</td>\n",
       "      <td>1.2859</td>\n",
       "      <td>19.3710</td>\n",
       "      <td>11.3702</td>\n",
       "      <td>0.7399</td>\n",
       "      <td>2.7995</td>\n",
       "      <td>5.8434</td>\n",
       "      <td>10.8160</td>\n",
       "      <td>3.6783</td>\n",
       "      <td>-11.1147</td>\n",
       "      <td>1.8730</td>\n",
       "      <td>9.8775</td>\n",
       "      <td>11.7842</td>\n",
       "      <td>1.2444</td>\n",
       "      <td>-47.3797</td>\n",
       "      <td>7.3718</td>\n",
       "      <td>0.1948</td>\n",
       "      <td>34.4014</td>\n",
       "      <td>25.7037</td>\n",
       "      <td>11.8343</td>\n",
       "      <td>13.2256</td>\n",
       "      <td>-4.1083</td>\n",
       "      <td>6.6885</td>\n",
       "      <td>-8.0946</td>\n",
       "      <td>18.5995</td>\n",
       "      <td>19.3219</td>\n",
       "      <td>7.0118</td>\n",
       "      <td>1.9210</td>\n",
       "      <td>8.8682</td>\n",
       "      <td>8.0109</td>\n",
       "      <td>-7.2417</td>\n",
       "      <td>1.7944</td>\n",
       "      <td>-1.3147</td>\n",
       "      <td>8.1042</td>\n",
       "      <td>1.5365</td>\n",
       "      <td>5.4007</td>\n",
       "      <td>7.9344</td>\n",
       "      <td>5.0220</td>\n",
       "      <td>2.2302</td>\n",
       "      <td>40.5632</td>\n",
       "      <td>0.5134</td>\n",
       "      <td>3.1701</td>\n",
       "      <td>20.1068</td>\n",
       "      <td>7.7841</td>\n",
       "      <td>7.0529</td>\n",
       "      <td>3.2709</td>\n",
       "      <td>23.4822</td>\n",
       "      <td>5.5075</td>\n",
       "      <td>13.7814</td>\n",
       "      <td>2.5462</td>\n",
       "      <td>18.1782</td>\n",
       "      <td>0.3683</td>\n",
       "      <td>-4.8210</td>\n",
       "      <td>-5.4850</td>\n",
       "      <td>13.7867</td>\n",
       "      <td>-13.5901</td>\n",
       "      <td>11.0993</td>\n",
       "      <td>7.9022</td>\n",
       "      <td>12.2301</td>\n",
       "      <td>0.4768</td>\n",
       "      <td>6.8852</td>\n",
       "      <td>8.0905</td>\n",
       "      <td>10.9631</td>\n",
       "      <td>11.7569</td>\n",
       "      <td>-1.2722</td>\n",
       "      <td>24.7876</td>\n",
       "      <td>26.6881</td>\n",
       "      <td>1.8944</td>\n",
       "      <td>0.6939</td>\n",
       "      <td>-13.6950</td>\n",
       "      <td>8.4068</td>\n",
       "      <td>35.4734</td>\n",
       "      <td>1.7093</td>\n",
       "      <td>15.1866</td>\n",
       "      <td>2.6227</td>\n",
       "      <td>7.3412</td>\n",
       "      <td>32.0888</td>\n",
       "      <td>13.9550</td>\n",
       "      <td>13.0858</td>\n",
       "      <td>6.6203</td>\n",
       "      <td>7.1051</td>\n",
       "      <td>5.3523</td>\n",
       "      <td>8.5426</td>\n",
       "      <td>3.6159</td>\n",
       "      <td>4.1569</td>\n",
       "      <td>3.0454</td>\n",
       "      <td>7.8522</td>\n",
       "      <td>-11.5100</td>\n",
       "      <td>7.5109</td>\n",
       "      <td>31.5899</td>\n",
       "      <td>9.5018</td>\n",
       "      <td>8.2736</td>\n",
       "      <td>10.1633</td>\n",
       "      <td>0.1225</td>\n",
       "      <td>12.5942</td>\n",
       "      <td>14.5697</td>\n",
       "      <td>2.4354</td>\n",
       "      <td>0.8194</td>\n",
       "      <td>16.5346</td>\n",
       "      <td>12.4205</td>\n",
       "      <td>-0.1780</td>\n",
       "      <td>5.7582</td>\n",
       "      <td>7.0513</td>\n",
       "      <td>1.9568</td>\n",
       "      <td>-8.9921</td>\n",
       "      <td>9.7797</td>\n",
       "      <td>18.1577</td>\n",
       "      <td>-1.9721</td>\n",
       "      <td>16.1622</td>\n",
       "      <td>3.6937</td>\n",
       "      <td>6.6803</td>\n",
       "      <td>-0.3243</td>\n",
       "      <td>12.2806</td>\n",
       "      <td>8.6086</td>\n",
       "      <td>11.0738</td>\n",
       "      <td>8.9231</td>\n",
       "      <td>11.7700</td>\n",
       "      <td>4.2578</td>\n",
       "      <td>-4.4223</td>\n",
       "      <td>20.6294</td>\n",
       "      <td>14.8743</td>\n",
       "      <td>9.4317</td>\n",
       "      <td>16.7242</td>\n",
       "      <td>-0.5687</td>\n",
       "      <td>0.1898</td>\n",
       "      <td>12.2419</td>\n",
       "      <td>-9.6953</td>\n",
       "      <td>22.3949</td>\n",
       "      <td>10.6261</td>\n",
       "      <td>29.4846</td>\n",
       "      <td>5.8683</td>\n",
       "      <td>3.8208</td>\n",
       "      <td>15.8348</td>\n",
       "      <td>-5.0121</td>\n",
       "      <td>15.1345</td>\n",
       "      <td>3.2003</td>\n",
       "      <td>9.3192</td>\n",
       "      <td>3.8821</td>\n",
       "      <td>5.7999</td>\n",
       "      <td>5.5378</td>\n",
       "      <td>5.0988</td>\n",
       "      <td>22.0330</td>\n",
       "      <td>5.5134</td>\n",
       "      <td>30.2645</td>\n",
       "      <td>10.4968</td>\n",
       "      <td>-7.2352</td>\n",
       "      <td>16.5721</td>\n",
       "      <td>-7.3477</td>\n",
       "      <td>11.0752</td>\n",
       "      <td>-5.5937</td>\n",
       "      <td>9.4878</td>\n",
       "      <td>-14.9100</td>\n",
       "      <td>9.4245</td>\n",
       "      <td>22.5441</td>\n",
       "      <td>-4.8622</td>\n",
       "      <td>7.6543</td>\n",
       "      <td>-15.9319</td>\n",
       "      <td>13.3175</td>\n",
       "      <td>-0.3566</td>\n",
       "      <td>7.6421</td>\n",
       "      <td>7.7214</td>\n",
       "      <td>2.5837</td>\n",
       "      <td>10.9516</td>\n",
       "      <td>15.4305</td>\n",
       "      <td>2.0339</td>\n",
       "      <td>8.1267</td>\n",
       "      <td>8.7889</td>\n",
       "      <td>18.3560</td>\n",
       "      <td>1.9518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>0</td>\n",
       "      <td>8.6093</td>\n",
       "      <td>-2.7457</td>\n",
       "      <td>12.0805</td>\n",
       "      <td>7.8928</td>\n",
       "      <td>10.5825</td>\n",
       "      <td>-9.0837</td>\n",
       "      <td>6.9427</td>\n",
       "      <td>14.6155</td>\n",
       "      <td>-4.9193</td>\n",
       "      <td>5.9525</td>\n",
       "      <td>-0.3249</td>\n",
       "      <td>-11.2648</td>\n",
       "      <td>14.1929</td>\n",
       "      <td>7.3124</td>\n",
       "      <td>7.5244</td>\n",
       "      <td>14.6472</td>\n",
       "      <td>7.6782</td>\n",
       "      <td>-1.7395</td>\n",
       "      <td>4.7011</td>\n",
       "      <td>20.4775</td>\n",
       "      <td>17.7559</td>\n",
       "      <td>18.1377</td>\n",
       "      <td>1.2145</td>\n",
       "      <td>3.5137</td>\n",
       "      <td>5.6777</td>\n",
       "      <td>13.2177</td>\n",
       "      <td>-7.9940</td>\n",
       "      <td>-2.9029</td>\n",
       "      <td>5.8463</td>\n",
       "      <td>6.1439</td>\n",
       "      <td>-11.1025</td>\n",
       "      <td>12.4858</td>\n",
       "      <td>-2.2871</td>\n",
       "      <td>19.0422</td>\n",
       "      <td>11.0449</td>\n",
       "      <td>4.1087</td>\n",
       "      <td>4.6974</td>\n",
       "      <td>6.9346</td>\n",
       "      <td>10.8917</td>\n",
       "      <td>0.9003</td>\n",
       "      <td>-13.5174</td>\n",
       "      <td>2.2439</td>\n",
       "      <td>11.5283</td>\n",
       "      <td>12.0406</td>\n",
       "      <td>4.1006</td>\n",
       "      <td>-7.9078</td>\n",
       "      <td>11.1405</td>\n",
       "      <td>-5.7864</td>\n",
       "      <td>20.7477</td>\n",
       "      <td>6.8874</td>\n",
       "      <td>12.9143</td>\n",
       "      <td>19.5856</td>\n",
       "      <td>0.7268</td>\n",
       "      <td>6.4059</td>\n",
       "      <td>9.3124</td>\n",
       "      <td>6.2846</td>\n",
       "      <td>15.6372</td>\n",
       "      <td>5.8200</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>9.1854</td>\n",
       "      <td>12.5963</td>\n",
       "      <td>-10.3734</td>\n",
       "      <td>0.8748</td>\n",
       "      <td>5.8042</td>\n",
       "      <td>3.7163</td>\n",
       "      <td>-1.1016</td>\n",
       "      <td>7.3667</td>\n",
       "      <td>9.8565</td>\n",
       "      <td>5.0228</td>\n",
       "      <td>-5.7828</td>\n",
       "      <td>2.3612</td>\n",
       "      <td>0.8520</td>\n",
       "      <td>6.3577</td>\n",
       "      <td>12.1719</td>\n",
       "      <td>19.7312</td>\n",
       "      <td>19.4465</td>\n",
       "      <td>4.5048</td>\n",
       "      <td>23.2378</td>\n",
       "      <td>6.3191</td>\n",
       "      <td>12.8046</td>\n",
       "      <td>7.4729</td>\n",
       "      <td>15.7811</td>\n",
       "      <td>13.3529</td>\n",
       "      <td>10.1852</td>\n",
       "      <td>5.4604</td>\n",
       "      <td>19.0773</td>\n",
       "      <td>-4.4577</td>\n",
       "      <td>9.5413</td>\n",
       "      <td>11.9052</td>\n",
       "      <td>2.1447</td>\n",
       "      <td>-22.4038</td>\n",
       "      <td>7.0883</td>\n",
       "      <td>14.1613</td>\n",
       "      <td>10.5080</td>\n",
       "      <td>14.2621</td>\n",
       "      <td>0.2647</td>\n",
       "      <td>20.4031</td>\n",
       "      <td>17.0360</td>\n",
       "      <td>1.6981</td>\n",
       "      <td>-0.0269</td>\n",
       "      <td>-0.3939</td>\n",
       "      <td>12.6317</td>\n",
       "      <td>14.8863</td>\n",
       "      <td>1.3854</td>\n",
       "      <td>15.0284</td>\n",
       "      <td>3.9995</td>\n",
       "      <td>5.3683</td>\n",
       "      <td>8.6273</td>\n",
       "      <td>14.1963</td>\n",
       "      <td>20.3882</td>\n",
       "      <td>3.2304</td>\n",
       "      <td>5.7033</td>\n",
       "      <td>4.5255</td>\n",
       "      <td>2.1929</td>\n",
       "      <td>3.1290</td>\n",
       "      <td>2.9044</td>\n",
       "      <td>1.1696</td>\n",
       "      <td>28.7632</td>\n",
       "      <td>-17.2738</td>\n",
       "      <td>2.1056</td>\n",
       "      <td>21.1613</td>\n",
       "      <td>8.9573</td>\n",
       "      <td>2.7768</td>\n",
       "      <td>-2.1746</td>\n",
       "      <td>3.6932</td>\n",
       "      <td>12.4653</td>\n",
       "      <td>14.1978</td>\n",
       "      <td>-2.5511</td>\n",
       "      <td>-0.9479</td>\n",
       "      <td>17.1092</td>\n",
       "      <td>11.5419</td>\n",
       "      <td>0.0975</td>\n",
       "      <td>8.8186</td>\n",
       "      <td>6.6231</td>\n",
       "      <td>3.9358</td>\n",
       "      <td>-11.7218</td>\n",
       "      <td>24.5437</td>\n",
       "      <td>15.5827</td>\n",
       "      <td>3.8212</td>\n",
       "      <td>8.6674</td>\n",
       "      <td>7.3834</td>\n",
       "      <td>-2.4438</td>\n",
       "      <td>10.2158</td>\n",
       "      <td>7.4844</td>\n",
       "      <td>9.1104</td>\n",
       "      <td>4.3649</td>\n",
       "      <td>11.4934</td>\n",
       "      <td>1.7624</td>\n",
       "      <td>4.0714</td>\n",
       "      <td>-1.2681</td>\n",
       "      <td>14.3330</td>\n",
       "      <td>8.0088</td>\n",
       "      <td>4.4015</td>\n",
       "      <td>14.1479</td>\n",
       "      <td>-5.1747</td>\n",
       "      <td>0.5778</td>\n",
       "      <td>14.5362</td>\n",
       "      <td>-1.7624</td>\n",
       "      <td>33.8820</td>\n",
       "      <td>11.6041</td>\n",
       "      <td>13.2070</td>\n",
       "      <td>5.8442</td>\n",
       "      <td>4.7086</td>\n",
       "      <td>5.7141</td>\n",
       "      <td>-1.0410</td>\n",
       "      <td>20.5092</td>\n",
       "      <td>3.2790</td>\n",
       "      <td>-5.5952</td>\n",
       "      <td>7.3176</td>\n",
       "      <td>5.7690</td>\n",
       "      <td>-7.0927</td>\n",
       "      <td>-3.9116</td>\n",
       "      <td>7.2569</td>\n",
       "      <td>-5.8234</td>\n",
       "      <td>25.6820</td>\n",
       "      <td>10.9202</td>\n",
       "      <td>-0.3104</td>\n",
       "      <td>8.8438</td>\n",
       "      <td>-9.7009</td>\n",
       "      <td>2.4013</td>\n",
       "      <td>-4.2935</td>\n",
       "      <td>9.3908</td>\n",
       "      <td>-13.2648</td>\n",
       "      <td>3.1545</td>\n",
       "      <td>23.0866</td>\n",
       "      <td>-5.3000</td>\n",
       "      <td>5.3745</td>\n",
       "      <td>-6.2660</td>\n",
       "      <td>10.1934</td>\n",
       "      <td>-0.8417</td>\n",
       "      <td>2.9057</td>\n",
       "      <td>9.7905</td>\n",
       "      <td>1.6704</td>\n",
       "      <td>1.6858</td>\n",
       "      <td>21.6042</td>\n",
       "      <td>3.1417</td>\n",
       "      <td>-6.5213</td>\n",
       "      <td>8.2675</td>\n",
       "      <td>14.7222</td>\n",
       "      <td>0.3965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0604</td>\n",
       "      <td>-2.1518</td>\n",
       "      <td>8.9522</td>\n",
       "      <td>7.1957</td>\n",
       "      <td>12.5846</td>\n",
       "      <td>-1.8361</td>\n",
       "      <td>5.8428</td>\n",
       "      <td>14.9250</td>\n",
       "      <td>-5.8609</td>\n",
       "      <td>8.2450</td>\n",
       "      <td>2.3061</td>\n",
       "      <td>2.8102</td>\n",
       "      <td>13.8463</td>\n",
       "      <td>11.9704</td>\n",
       "      <td>6.4569</td>\n",
       "      <td>14.8372</td>\n",
       "      <td>10.7430</td>\n",
       "      <td>-0.4299</td>\n",
       "      <td>15.9426</td>\n",
       "      <td>13.7257</td>\n",
       "      <td>20.3010</td>\n",
       "      <td>12.5579</td>\n",
       "      <td>6.8202</td>\n",
       "      <td>2.7229</td>\n",
       "      <td>12.1354</td>\n",
       "      <td>13.7367</td>\n",
       "      <td>0.8135</td>\n",
       "      <td>-0.9059</td>\n",
       "      <td>5.9070</td>\n",
       "      <td>2.8407</td>\n",
       "      <td>-15.2398</td>\n",
       "      <td>10.4407</td>\n",
       "      <td>-2.5731</td>\n",
       "      <td>6.1796</td>\n",
       "      <td>10.6093</td>\n",
       "      <td>-5.9158</td>\n",
       "      <td>8.1723</td>\n",
       "      <td>2.8521</td>\n",
       "      <td>9.1738</td>\n",
       "      <td>0.6665</td>\n",
       "      <td>-3.8294</td>\n",
       "      <td>-1.0370</td>\n",
       "      <td>11.7770</td>\n",
       "      <td>11.2834</td>\n",
       "      <td>8.0485</td>\n",
       "      <td>-24.6840</td>\n",
       "      <td>12.7404</td>\n",
       "      <td>-35.1659</td>\n",
       "      <td>0.7613</td>\n",
       "      <td>8.3838</td>\n",
       "      <td>12.6832</td>\n",
       "      <td>9.5503</td>\n",
       "      <td>1.7895</td>\n",
       "      <td>5.2091</td>\n",
       "      <td>8.0913</td>\n",
       "      <td>12.3972</td>\n",
       "      <td>14.4698</td>\n",
       "      <td>6.5850</td>\n",
       "      <td>3.3164</td>\n",
       "      <td>9.4638</td>\n",
       "      <td>15.7820</td>\n",
       "      <td>-25.0222</td>\n",
       "      <td>3.4418</td>\n",
       "      <td>-4.3923</td>\n",
       "      <td>8.6464</td>\n",
       "      <td>6.3072</td>\n",
       "      <td>5.6221</td>\n",
       "      <td>23.6143</td>\n",
       "      <td>5.0220</td>\n",
       "      <td>-3.9989</td>\n",
       "      <td>4.0462</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>1.2516</td>\n",
       "      <td>24.4187</td>\n",
       "      <td>4.5290</td>\n",
       "      <td>15.4235</td>\n",
       "      <td>11.6875</td>\n",
       "      <td>23.6273</td>\n",
       "      <td>4.0806</td>\n",
       "      <td>15.2733</td>\n",
       "      <td>0.7839</td>\n",
       "      <td>10.5404</td>\n",
       "      <td>1.6212</td>\n",
       "      <td>-5.2896</td>\n",
       "      <td>1.6027</td>\n",
       "      <td>17.9762</td>\n",
       "      <td>-2.3174</td>\n",
       "      <td>15.6298</td>\n",
       "      <td>4.5474</td>\n",
       "      <td>7.5509</td>\n",
       "      <td>-7.5866</td>\n",
       "      <td>7.0364</td>\n",
       "      <td>14.4027</td>\n",
       "      <td>10.7795</td>\n",
       "      <td>7.2887</td>\n",
       "      <td>-1.0930</td>\n",
       "      <td>11.3596</td>\n",
       "      <td>18.1486</td>\n",
       "      <td>2.8344</td>\n",
       "      <td>1.9480</td>\n",
       "      <td>-19.8592</td>\n",
       "      <td>22.5316</td>\n",
       "      <td>18.6129</td>\n",
       "      <td>1.3512</td>\n",
       "      <td>9.3291</td>\n",
       "      <td>4.2835</td>\n",
       "      <td>10.3907</td>\n",
       "      <td>7.0874</td>\n",
       "      <td>14.3256</td>\n",
       "      <td>14.4135</td>\n",
       "      <td>4.2827</td>\n",
       "      <td>6.9750</td>\n",
       "      <td>1.6480</td>\n",
       "      <td>11.6896</td>\n",
       "      <td>2.5762</td>\n",
       "      <td>-2.5459</td>\n",
       "      <td>5.3446</td>\n",
       "      <td>38.1015</td>\n",
       "      <td>3.5732</td>\n",
       "      <td>5.0988</td>\n",
       "      <td>30.5644</td>\n",
       "      <td>11.3025</td>\n",
       "      <td>3.9618</td>\n",
       "      <td>-8.2464</td>\n",
       "      <td>2.7038</td>\n",
       "      <td>12.3441</td>\n",
       "      <td>12.5431</td>\n",
       "      <td>-1.3683</td>\n",
       "      <td>3.5974</td>\n",
       "      <td>13.9761</td>\n",
       "      <td>14.3003</td>\n",
       "      <td>1.0486</td>\n",
       "      <td>8.9500</td>\n",
       "      <td>7.1954</td>\n",
       "      <td>-1.1984</td>\n",
       "      <td>1.9586</td>\n",
       "      <td>27.5609</td>\n",
       "      <td>24.6065</td>\n",
       "      <td>-2.8233</td>\n",
       "      <td>8.9821</td>\n",
       "      <td>3.8873</td>\n",
       "      <td>15.9638</td>\n",
       "      <td>10.0142</td>\n",
       "      <td>7.8388</td>\n",
       "      <td>9.9718</td>\n",
       "      <td>2.9253</td>\n",
       "      <td>10.4994</td>\n",
       "      <td>4.1622</td>\n",
       "      <td>3.7613</td>\n",
       "      <td>2.3701</td>\n",
       "      <td>18.0984</td>\n",
       "      <td>17.1765</td>\n",
       "      <td>7.6508</td>\n",
       "      <td>18.2452</td>\n",
       "      <td>17.0336</td>\n",
       "      <td>-10.9370</td>\n",
       "      <td>12.0500</td>\n",
       "      <td>-1.2155</td>\n",
       "      <td>19.9750</td>\n",
       "      <td>12.3892</td>\n",
       "      <td>31.8833</td>\n",
       "      <td>5.9684</td>\n",
       "      <td>7.2084</td>\n",
       "      <td>3.8899</td>\n",
       "      <td>-11.0882</td>\n",
       "      <td>17.2502</td>\n",
       "      <td>2.5881</td>\n",
       "      <td>-2.7018</td>\n",
       "      <td>0.5641</td>\n",
       "      <td>5.3430</td>\n",
       "      <td>-7.1541</td>\n",
       "      <td>-6.1920</td>\n",
       "      <td>18.2366</td>\n",
       "      <td>11.7134</td>\n",
       "      <td>14.7483</td>\n",
       "      <td>8.1013</td>\n",
       "      <td>11.8771</td>\n",
       "      <td>13.9552</td>\n",
       "      <td>-10.4701</td>\n",
       "      <td>5.6961</td>\n",
       "      <td>-3.7546</td>\n",
       "      <td>8.4117</td>\n",
       "      <td>1.8986</td>\n",
       "      <td>7.2601</td>\n",
       "      <td>-0.4639</td>\n",
       "      <td>-0.0498</td>\n",
       "      <td>7.9336</td>\n",
       "      <td>-12.8279</td>\n",
       "      <td>12.4124</td>\n",
       "      <td>1.8489</td>\n",
       "      <td>4.4666</td>\n",
       "      <td>4.7433</td>\n",
       "      <td>0.7178</td>\n",
       "      <td>1.4214</td>\n",
       "      <td>23.0347</td>\n",
       "      <td>-1.2706</td>\n",
       "      <td>-2.9275</td>\n",
       "      <td>10.2922</td>\n",
       "      <td>17.9697</td>\n",
       "      <td>-8.9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>0</td>\n",
       "      <td>9.8369</td>\n",
       "      <td>-1.4834</td>\n",
       "      <td>12.8746</td>\n",
       "      <td>6.6375</td>\n",
       "      <td>12.2772</td>\n",
       "      <td>2.4486</td>\n",
       "      <td>5.9405</td>\n",
       "      <td>19.2514</td>\n",
       "      <td>6.2654</td>\n",
       "      <td>7.6784</td>\n",
       "      <td>-9.4458</td>\n",
       "      <td>-12.1419</td>\n",
       "      <td>13.8481</td>\n",
       "      <td>7.8895</td>\n",
       "      <td>7.7894</td>\n",
       "      <td>15.0553</td>\n",
       "      <td>8.4871</td>\n",
       "      <td>-3.0680</td>\n",
       "      <td>6.5263</td>\n",
       "      <td>11.3152</td>\n",
       "      <td>21.4246</td>\n",
       "      <td>18.9608</td>\n",
       "      <td>10.1102</td>\n",
       "      <td>2.7142</td>\n",
       "      <td>14.2080</td>\n",
       "      <td>13.5433</td>\n",
       "      <td>3.1736</td>\n",
       "      <td>-3.3423</td>\n",
       "      <td>5.9015</td>\n",
       "      <td>7.9352</td>\n",
       "      <td>-3.1582</td>\n",
       "      <td>9.4668</td>\n",
       "      <td>-0.0083</td>\n",
       "      <td>19.3239</td>\n",
       "      <td>12.4057</td>\n",
       "      <td>0.6329</td>\n",
       "      <td>2.7922</td>\n",
       "      <td>5.8184</td>\n",
       "      <td>19.3038</td>\n",
       "      <td>1.4450</td>\n",
       "      <td>-5.5963</td>\n",
       "      <td>14.0685</td>\n",
       "      <td>11.9171</td>\n",
       "      <td>11.5111</td>\n",
       "      <td>6.9087</td>\n",
       "      <td>-65.4863</td>\n",
       "      <td>13.8657</td>\n",
       "      <td>0.0444</td>\n",
       "      <td>-0.1346</td>\n",
       "      <td>14.4268</td>\n",
       "      <td>13.3273</td>\n",
       "      <td>10.4857</td>\n",
       "      <td>-1.4367</td>\n",
       "      <td>5.7555</td>\n",
       "      <td>-8.5414</td>\n",
       "      <td>14.1482</td>\n",
       "      <td>16.9840</td>\n",
       "      <td>6.1812</td>\n",
       "      <td>1.9548</td>\n",
       "      <td>9.2048</td>\n",
       "      <td>8.6591</td>\n",
       "      <td>-27.7439</td>\n",
       "      <td>-0.4952</td>\n",
       "      <td>-1.7839</td>\n",
       "      <td>5.2670</td>\n",
       "      <td>-4.3205</td>\n",
       "      <td>6.9860</td>\n",
       "      <td>1.6184</td>\n",
       "      <td>5.0301</td>\n",
       "      <td>-3.2431</td>\n",
       "      <td>40.1236</td>\n",
       "      <td>0.7737</td>\n",
       "      <td>-0.7264</td>\n",
       "      <td>4.5886</td>\n",
       "      <td>-4.5346</td>\n",
       "      <td>23.3521</td>\n",
       "      <td>1.0273</td>\n",
       "      <td>19.1600</td>\n",
       "      <td>7.1734</td>\n",
       "      <td>14.3937</td>\n",
       "      <td>2.9598</td>\n",
       "      <td>13.3317</td>\n",
       "      <td>-9.2587</td>\n",
       "      <td>-6.7075</td>\n",
       "      <td>7.8984</td>\n",
       "      <td>14.5265</td>\n",
       "      <td>7.0799</td>\n",
       "      <td>20.1670</td>\n",
       "      <td>8.0053</td>\n",
       "      <td>3.7954</td>\n",
       "      <td>-39.7997</td>\n",
       "      <td>7.0065</td>\n",
       "      <td>9.3627</td>\n",
       "      <td>10.4316</td>\n",
       "      <td>14.0553</td>\n",
       "      <td>0.0213</td>\n",
       "      <td>14.7246</td>\n",
       "      <td>35.2988</td>\n",
       "      <td>1.6844</td>\n",
       "      <td>0.6715</td>\n",
       "      <td>-22.9264</td>\n",
       "      <td>12.3562</td>\n",
       "      <td>17.3410</td>\n",
       "      <td>1.6940</td>\n",
       "      <td>7.1179</td>\n",
       "      <td>5.1934</td>\n",
       "      <td>8.8230</td>\n",
       "      <td>10.6617</td>\n",
       "      <td>14.0837</td>\n",
       "      <td>28.2749</td>\n",
       "      <td>-0.1937</td>\n",
       "      <td>5.9654</td>\n",
       "      <td>1.0719</td>\n",
       "      <td>7.9923</td>\n",
       "      <td>2.9138</td>\n",
       "      <td>-3.6135</td>\n",
       "      <td>1.4684</td>\n",
       "      <td>25.6795</td>\n",
       "      <td>13.8224</td>\n",
       "      <td>4.7478</td>\n",
       "      <td>41.1037</td>\n",
       "      <td>12.7140</td>\n",
       "      <td>5.2964</td>\n",
       "      <td>9.7289</td>\n",
       "      <td>3.9370</td>\n",
       "      <td>12.1316</td>\n",
       "      <td>12.5815</td>\n",
       "      <td>7.0642</td>\n",
       "      <td>5.6518</td>\n",
       "      <td>10.9346</td>\n",
       "      <td>11.4266</td>\n",
       "      <td>0.9442</td>\n",
       "      <td>7.7532</td>\n",
       "      <td>6.6173</td>\n",
       "      <td>-6.8304</td>\n",
       "      <td>6.4730</td>\n",
       "      <td>17.1728</td>\n",
       "      <td>25.8128</td>\n",
       "      <td>2.6791</td>\n",
       "      <td>13.9547</td>\n",
       "      <td>6.6289</td>\n",
       "      <td>-4.3965</td>\n",
       "      <td>11.7159</td>\n",
       "      <td>16.1080</td>\n",
       "      <td>7.6874</td>\n",
       "      <td>9.1570</td>\n",
       "      <td>11.5670</td>\n",
       "      <td>-12.7047</td>\n",
       "      <td>3.7574</td>\n",
       "      <td>9.9110</td>\n",
       "      <td>20.1461</td>\n",
       "      <td>1.2995</td>\n",
       "      <td>5.8493</td>\n",
       "      <td>19.8234</td>\n",
       "      <td>4.7022</td>\n",
       "      <td>10.6101</td>\n",
       "      <td>13.0021</td>\n",
       "      <td>-12.6068</td>\n",
       "      <td>27.0846</td>\n",
       "      <td>8.0913</td>\n",
       "      <td>33.5107</td>\n",
       "      <td>5.6953</td>\n",
       "      <td>5.4663</td>\n",
       "      <td>18.2201</td>\n",
       "      <td>6.5769</td>\n",
       "      <td>21.2607</td>\n",
       "      <td>3.2304</td>\n",
       "      <td>-1.7759</td>\n",
       "      <td>3.1283</td>\n",
       "      <td>5.5518</td>\n",
       "      <td>1.4493</td>\n",
       "      <td>-2.6627</td>\n",
       "      <td>19.8056</td>\n",
       "      <td>2.3705</td>\n",
       "      <td>18.4685</td>\n",
       "      <td>16.3309</td>\n",
       "      <td>-3.3456</td>\n",
       "      <td>13.5261</td>\n",
       "      <td>1.7189</td>\n",
       "      <td>5.1743</td>\n",
       "      <td>-7.6938</td>\n",
       "      <td>9.7685</td>\n",
       "      <td>4.8910</td>\n",
       "      <td>12.2198</td>\n",
       "      <td>11.8503</td>\n",
       "      <td>-7.8931</td>\n",
       "      <td>6.4209</td>\n",
       "      <td>5.9270</td>\n",
       "      <td>16.0201</td>\n",
       "      <td>-0.2829</td>\n",
       "      <td>-1.4905</td>\n",
       "      <td>9.5214</td>\n",
       "      <td>-0.1508</td>\n",
       "      <td>9.1942</td>\n",
       "      <td>13.2876</td>\n",
       "      <td>-1.5121</td>\n",
       "      <td>3.9267</td>\n",
       "      <td>9.5031</td>\n",
       "      <td>17.9974</td>\n",
       "      <td>-8.8104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_code  target    var_0   var_1    var_2   var_3    var_4   var_5   var_6  \\\n",
       "0  train_0       0   8.9255 -6.7863  11.9081  5.0930  11.4607 -9.2834  5.1187   \n",
       "1  train_1       0  11.5006 -4.1473  13.8588  5.3890  12.3622  7.0433  5.6208   \n",
       "2  train_2       0   8.6093 -2.7457  12.0805  7.8928  10.5825 -9.0837  6.9427   \n",
       "3  train_3       0  11.0604 -2.1518   8.9522  7.1957  12.5846 -1.8361  5.8428   \n",
       "4  train_4       0   9.8369 -1.4834  12.8746  6.6375  12.2772  2.4486  5.9405   \n",
       "\n",
       "     var_7   var_8   var_9  var_10   var_11   var_12   var_13  var_14  \\\n",
       "0  18.6266 -4.9200  5.7470  2.9252   3.1821  14.0137   0.5745  8.7989   \n",
       "1  16.5338  3.1468  8.0851 -0.4032   8.0585  14.0239   8.4135  5.4345   \n",
       "2  14.6155 -4.9193  5.9525 -0.3249 -11.2648  14.1929   7.3124  7.5244   \n",
       "3  14.9250 -5.8609  8.2450  2.3061   2.8102  13.8463  11.9704  6.4569   \n",
       "4  19.2514  6.2654  7.6784 -9.4458 -12.1419  13.8481   7.8895  7.7894   \n",
       "\n",
       "    var_15   var_16   var_17   var_18   var_19   var_20   var_21   var_22  \\\n",
       "0  14.5691   5.7487  -7.2393   4.2840  30.7133  10.5350  16.2191   2.5791   \n",
       "1  13.7003  13.8275 -15.5849   7.8000  28.5708   3.4287   2.7407   8.5524   \n",
       "2  14.6472   7.6782  -1.7395   4.7011  20.4775  17.7559  18.1377   1.2145   \n",
       "3  14.8372  10.7430  -0.4299  15.9426  13.7257  20.3010  12.5579   6.8202   \n",
       "4  15.0553   8.4871  -3.0680   6.5263  11.3152  21.4246  18.9608  10.1102   \n",
       "\n",
       "   var_23   var_24   var_25   var_26  var_27  var_28  var_29   var_30  \\\n",
       "0  2.4716  14.3831  13.4325  -5.1488 -0.4073  4.9306  5.9965  -0.3085   \n",
       "1  3.3716   6.9779  13.8910 -11.7684 -2.5586  5.0464  0.5481  -9.2987   \n",
       "2  3.5137   5.6777  13.2177  -7.9940 -2.9029  5.8463  6.1439 -11.1025   \n",
       "3  2.7229  12.1354  13.7367   0.8135 -0.9059  5.9070  2.8407 -15.2398   \n",
       "4  2.7142  14.2080  13.5433   3.1736 -3.3423  5.9015  7.9352  -3.1582   \n",
       "\n",
       "    var_31  var_32   var_33   var_34   var_35  var_36  var_37   var_38  \\\n",
       "0  12.9041 -3.8766  16.8911  11.1920  10.5785  0.6764  7.8871   4.6667   \n",
       "1   7.8755  1.2859  19.3710  11.3702   0.7399  2.7995  5.8434  10.8160   \n",
       "2  12.4858 -2.2871  19.0422  11.0449   4.1087  4.6974  6.9346  10.8917   \n",
       "3  10.4407 -2.5731   6.1796  10.6093  -5.9158  8.1723  2.8521   9.1738   \n",
       "4   9.4668 -0.0083  19.3239  12.4057   0.6329  2.7922  5.8184  19.3038   \n",
       "\n",
       "   var_39   var_40   var_41   var_42   var_43   var_44   var_45   var_46  \\\n",
       "0  3.8743  -5.2387   7.3746  11.5767  12.0446  11.6418  -7.0170   5.9226   \n",
       "1  3.6783 -11.1147   1.8730   9.8775  11.7842   1.2444 -47.3797   7.3718   \n",
       "2  0.9003 -13.5174   2.2439  11.5283  12.0406   4.1006  -7.9078  11.1405   \n",
       "3  0.6665  -3.8294  -1.0370  11.7770  11.2834   8.0485 -24.6840  12.7404   \n",
       "4  1.4450  -5.5963  14.0685  11.9171  11.5111   6.9087 -65.4863  13.8657   \n",
       "\n",
       "    var_47   var_48   var_49   var_50   var_51  var_52  var_53  var_54  \\\n",
       "0 -14.2136  16.0283   5.3253  12.9194  29.0460 -0.6940  5.1736 -0.7474   \n",
       "1   0.1948  34.4014  25.7037  11.8343  13.2256 -4.1083  6.6885 -8.0946   \n",
       "2  -5.7864  20.7477   6.8874  12.9143  19.5856  0.7268  6.4059  9.3124   \n",
       "3 -35.1659   0.7613   8.3838  12.6832   9.5503  1.7895  5.2091  8.0913   \n",
       "4   0.0444  -0.1346  14.4268  13.3273  10.4857 -1.4367  5.7555 -8.5414   \n",
       "\n",
       "    var_55   var_56  var_57  var_58   var_59   var_60   var_61  var_62  \\\n",
       "0  14.8322  11.2668  5.3822  2.0183  10.1166  16.1828   4.9590  2.0771   \n",
       "1  18.5995  19.3219  7.0118  1.9210   8.8682   8.0109  -7.2417  1.7944   \n",
       "2   6.2846  15.6372  5.8200  1.1000   9.1854  12.5963 -10.3734  0.8748   \n",
       "3  12.3972  14.4698  6.5850  3.3164   9.4638  15.7820 -25.0222  3.4418   \n",
       "4  14.1482  16.9840  6.1812  1.9548   9.2048   8.6591 -27.7439 -0.4952   \n",
       "\n",
       "   var_63  var_64  var_65  var_66   var_67  var_68  var_69   var_70  var_71  \\\n",
       "0 -0.2154  8.6748  9.5319  5.8056  22.4321  5.0109 -4.7010  21.6374  0.5663   \n",
       "1 -1.3147  8.1042  1.5365  5.4007   7.9344  5.0220  2.2302  40.5632  0.5134   \n",
       "2  5.8042  3.7163 -1.1016  7.3667   9.8565  5.0228 -5.7828   2.3612  0.8520   \n",
       "3 -4.3923  8.6464  6.3072  5.6221  23.6143  5.0220 -3.9989   4.0462  0.2500   \n",
       "4 -1.7839  5.2670 -4.3205  6.9860   1.6184  5.0301 -3.2431  40.1236  0.7737   \n",
       "\n",
       "   var_72   var_73   var_74   var_75   var_76   var_77  var_78   var_79  \\\n",
       "0  5.1999   8.8600  43.1127  18.3816  -2.3440  23.4104  6.5199  12.1983   \n",
       "1  3.1701  20.1068   7.7841   7.0529   3.2709  23.4822  5.5075  13.7814   \n",
       "2  6.3577  12.1719  19.7312  19.4465   4.5048  23.2378  6.3191  12.8046   \n",
       "3  1.2516  24.4187   4.5290  15.4235  11.6875  23.6273  4.0806  15.2733   \n",
       "4 -0.7264   4.5886  -4.5346  23.3521   1.0273  19.1600  7.1734  14.3937   \n",
       "\n",
       "    var_80   var_81   var_82   var_83  var_84   var_85   var_86   var_87  \\\n",
       "0  13.6468  13.8372   1.3675   2.9423 -4.5213  21.4669   9.3225  16.4597   \n",
       "1   2.5462  18.1782   0.3683  -4.8210 -5.4850  13.7867 -13.5901  11.0993   \n",
       "2   7.4729  15.7811  13.3529  10.1852  5.4604  19.0773  -4.4577   9.5413   \n",
       "3   0.7839  10.5404   1.6212  -5.2896  1.6027  17.9762  -2.3174  15.6298   \n",
       "4   2.9598  13.3317  -9.2587  -6.7075  7.8984  14.5265   7.0799  20.1670   \n",
       "\n",
       "    var_88   var_89   var_90  var_91   var_92   var_93   var_94  var_95  \\\n",
       "0   7.9984  -1.7069 -21.4494  6.7806  11.0924   9.9913  14.8421  0.1812   \n",
       "1   7.9022  12.2301   0.4768  6.8852   8.0905  10.9631  11.7569 -1.2722   \n",
       "2  11.9052   2.1447 -22.4038  7.0883  14.1613  10.5080  14.2621  0.2647   \n",
       "3   4.5474   7.5509  -7.5866  7.0364  14.4027  10.7795   7.2887 -1.0930   \n",
       "4   8.0053   3.7954 -39.7997  7.0065   9.3627  10.4316  14.0553  0.0213   \n",
       "\n",
       "    var_96   var_97  var_98  var_99  var_100  var_101  var_102  var_103  \\\n",
       "0   8.9642  16.2572  2.1743 -3.4132   9.4763  13.3102  26.5376   1.4403   \n",
       "1  24.7876  26.6881  1.8944  0.6939 -13.6950   8.4068  35.4734   1.7093   \n",
       "2  20.4031  17.0360  1.6981 -0.0269  -0.3939  12.6317  14.8863   1.3854   \n",
       "3  11.3596  18.1486  2.8344  1.9480 -19.8592  22.5316  18.6129   1.3512   \n",
       "4  14.7246  35.2988  1.6844  0.6715 -22.9264  12.3562  17.3410   1.6940   \n",
       "\n",
       "   var_104  var_105  var_106  var_107  var_108  var_109  var_110  var_111  \\\n",
       "0  14.7100   6.0454   9.5426  17.1554  14.1104  24.3627   2.0323   6.7602   \n",
       "1  15.1866   2.6227   7.3412  32.0888  13.9550  13.0858   6.6203   7.1051   \n",
       "2  15.0284   3.9995   5.3683   8.6273  14.1963  20.3882   3.2304   5.7033   \n",
       "3   9.3291   4.2835  10.3907   7.0874  14.3256  14.4135   4.2827   6.9750   \n",
       "4   7.1179   5.1934   8.8230  10.6617  14.0837  28.2749  -0.1937   5.9654   \n",
       "\n",
       "   var_112  var_113  var_114  var_115  var_116  var_117  var_118  var_119  \\\n",
       "0   3.9141  -0.4851   2.5240   1.5093   2.5516  15.5752 -13.4221   7.2739   \n",
       "1   5.3523   8.5426   3.6159   4.1569   3.0454   7.8522 -11.5100   7.5109   \n",
       "2   4.5255   2.1929   3.1290   2.9044   1.1696  28.7632 -17.2738   2.1056   \n",
       "3   1.6480  11.6896   2.5762  -2.5459   5.3446  38.1015   3.5732   5.0988   \n",
       "4   1.0719   7.9923   2.9138  -3.6135   1.4684  25.6795  13.8224   4.7478   \n",
       "\n",
       "   var_120  var_121  var_122  var_123  var_124  var_125  var_126  var_127  \\\n",
       "0  16.0094   9.7268   0.8897   0.7754   4.2218  12.0039  13.8571  -0.7338   \n",
       "1  31.5899   9.5018   8.2736  10.1633   0.1225  12.5942  14.5697   2.4354   \n",
       "2  21.1613   8.9573   2.7768  -2.1746   3.6932  12.4653  14.1978  -2.5511   \n",
       "3  30.5644  11.3025   3.9618  -8.2464   2.7038  12.3441  12.5431  -1.3683   \n",
       "4  41.1037  12.7140   5.2964   9.7289   3.9370  12.1316  12.5815   7.0642   \n",
       "\n",
       "   var_128  var_129  var_130  var_131  var_132  var_133  var_134  var_135  \\\n",
       "0  -1.9245  15.4462  12.8287   0.3587   9.6508   6.5674   5.1726   3.1345   \n",
       "1   0.8194  16.5346  12.4205  -0.1780   5.7582   7.0513   1.9568  -8.9921   \n",
       "2  -0.9479  17.1092  11.5419   0.0975   8.8186   6.6231   3.9358 -11.7218   \n",
       "3   3.5974  13.9761  14.3003   1.0486   8.9500   7.1954  -1.1984   1.9586   \n",
       "4   5.6518  10.9346  11.4266   0.9442   7.7532   6.6173  -6.8304   6.4730   \n",
       "\n",
       "   var_136  var_137  var_138  var_139  var_140  var_141  var_142  var_143  \\\n",
       "0  29.4547  31.4045   2.8279  15.6599   8.3307  -5.6011  19.0614  11.2663   \n",
       "1   9.7797  18.1577  -1.9721  16.1622   3.6937   6.6803  -0.3243  12.2806   \n",
       "2  24.5437  15.5827   3.8212   8.6674   7.3834  -2.4438  10.2158   7.4844   \n",
       "3  27.5609  24.6065  -2.8233   8.9821   3.8873  15.9638  10.0142   7.8388   \n",
       "4  17.1728  25.8128   2.6791  13.9547   6.6289  -4.3965  11.7159  16.1080   \n",
       "\n",
       "   var_144  var_145  var_146  var_147  var_148  var_149  var_150  var_151  \\\n",
       "0   8.6989   8.3694  11.5659 -16.4727   4.0288  17.9244  18.5177  10.7800   \n",
       "1   8.6086  11.0738   8.9231  11.7700   4.2578  -4.4223  20.6294  14.8743   \n",
       "2   9.1104   4.3649  11.4934   1.7624   4.0714  -1.2681  14.3330   8.0088   \n",
       "3   9.9718   2.9253  10.4994   4.1622   3.7613   2.3701  18.0984  17.1765   \n",
       "4   7.6874   9.1570  11.5670 -12.7047   3.7574   9.9110  20.1461   1.2995   \n",
       "\n",
       "   var_152  var_153  var_154  var_155  var_156  var_157  var_158  var_159  \\\n",
       "0   9.0056  16.6964  10.4838   1.6573  12.1749 -13.1324  17.6054  11.5423   \n",
       "1   9.4317  16.7242  -0.5687   0.1898  12.2419  -9.6953  22.3949  10.6261   \n",
       "2   4.4015  14.1479  -5.1747   0.5778  14.5362  -1.7624  33.8820  11.6041   \n",
       "3   7.6508  18.2452  17.0336 -10.9370  12.0500  -1.2155  19.9750  12.3892   \n",
       "4   5.8493  19.8234   4.7022  10.6101  13.0021 -12.6068  27.0846   8.0913   \n",
       "\n",
       "   var_160  var_161  var_162  var_163  var_164  var_165  var_166  var_167  \\\n",
       "0  15.4576   5.3133   3.6159   5.0384   6.6760  12.6644   2.7004  -0.6975   \n",
       "1  29.4846   5.8683   3.8208  15.8348  -5.0121  15.1345   3.2003   9.3192   \n",
       "2  13.2070   5.8442   4.7086   5.7141  -1.0410  20.5092   3.2790  -5.5952   \n",
       "3  31.8833   5.9684   7.2084   3.8899 -11.0882  17.2502   2.5881  -2.7018   \n",
       "4  33.5107   5.6953   5.4663  18.2201   6.5769  21.2607   3.2304  -1.7759   \n",
       "\n",
       "   var_168  var_169  var_170  var_171  var_172  var_173  var_174  var_175  \\\n",
       "0   9.5981   5.4879  -4.7645  -8.4254  20.8773   3.1531  18.5618   7.7423   \n",
       "1   3.8821   5.7999   5.5378   5.0988  22.0330   5.5134  30.2645  10.4968   \n",
       "2   7.3176   5.7690  -7.0927  -3.9116   7.2569  -5.8234  25.6820  10.9202   \n",
       "3   0.5641   5.3430  -7.1541  -6.1920  18.2366  11.7134  14.7483   8.1013   \n",
       "4   3.1283   5.5518   1.4493  -2.6627  19.8056   2.3705  18.4685  16.3309   \n",
       "\n",
       "   var_176  var_177  var_178  var_179  var_180  var_181  var_182  var_183  \\\n",
       "0 -10.1245  13.7241  -3.5189   1.7202  -8.4051   9.0164   3.0657  14.3691   \n",
       "1  -7.2352  16.5721  -7.3477  11.0752  -5.5937   9.4878 -14.9100   9.4245   \n",
       "2  -0.3104   8.8438  -9.7009   2.4013  -4.2935   9.3908 -13.2648   3.1545   \n",
       "3  11.8771  13.9552 -10.4701   5.6961  -3.7546   8.4117   1.8986   7.2601   \n",
       "4  -3.3456  13.5261   1.7189   5.1743  -7.6938   9.7685   4.8910  12.2198   \n",
       "\n",
       "   var_184  var_185  var_186  var_187  var_188  var_189  var_190  var_191  \\\n",
       "0  25.8398   5.8764  11.8411 -19.7159  17.5743   0.5857   4.4354   3.9642   \n",
       "1  22.5441  -4.8622   7.6543 -15.9319  13.3175  -0.3566   7.6421   7.7214   \n",
       "2  23.0866  -5.3000   5.3745  -6.2660  10.1934  -0.8417   2.9057   9.7905   \n",
       "3  -0.4639  -0.0498   7.9336 -12.8279  12.4124   1.8489   4.4666   4.7433   \n",
       "4  11.8503  -7.8931   6.4209   5.9270  16.0201  -0.2829  -1.4905   9.5214   \n",
       "\n",
       "   var_192  var_193  var_194  var_195  var_196  var_197  var_198  var_199  \n",
       "0   3.1364   1.6910  18.5227  -2.3978   7.8784   8.5635  12.7803  -1.0914  \n",
       "1   2.5837  10.9516  15.4305   2.0339   8.1267   8.7889  18.3560   1.9518  \n",
       "2   1.6704   1.6858  21.6042   3.1417  -6.5213   8.2675  14.7222   0.3965  \n",
       "3   0.7178   1.4214  23.0347  -1.2706  -2.9275  10.2922  17.9697  -8.9996  \n",
       "4  -0.1508   9.1942  13.2876  -1.5121   3.9267   9.5031  17.9974  -8.8104  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data_raw/train.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funkcje pomocnicze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T20:52:38.016142Z",
     "start_time": "2019-07-17T20:52:38.004151Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_feats(df): #dzieki temu nie będę brał niepotrzebnych kolumn do modelowania\n",
    "    feats = [f for f in df.columns if f not in ['ID_code','target']]\n",
    "    return feats\n",
    "\n",
    "def get_X(df): #do pobierania macierzy X czyli cech które będę starał się opisywac modelem\n",
    "    return df[ get_feats(df) ].values\n",
    "\n",
    "def get_y(df, target_var='target'): #wektor y - informacja o predycji. w przypadku przykładowych danych to \":target\"\n",
    "    return df[target_var].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Podział zbioru do nauki modelu, testowy i walidacyjny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T20:52:39.936023Z",
     "start_time": "2019-07-17T20:52:38.727694Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: (120000, 200) (120000,)\n",
      "TEST: (40000, 200) (40000,)\n",
      "VALIDATION: (40000, 200) (40000,)\n"
     ]
    }
   ],
   "source": [
    "X, y = get_X(df), get_y(df) \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=2019)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=2019)\n",
    "\n",
    "print('TRAIN:',X_train.shape, y_train.shape)\n",
    "print('TEST:',X_test.shape, y_test.shape)\n",
    "print('VALIDATION:',X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Walidacja modelu\n",
    "Zdefiniujmy sobie funkcję dzięki którym od razu sprawdzimy jak model działa.<br>\n",
    "W tym przypadku przyglądnijmy się metryce GINI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T21:08:21.079376Z",
     "start_time": "2019-07-16T21:08:21.068369Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_measures(y,y_pred): \n",
    "    score_test = roc_auc_score(y, y_pred)\n",
    "    Gini_index = 2*score_test - 1\n",
    "    \n",
    "    d = {'AUC': [round(score_test,4)], 'GINI': [round(Gini_index,4)]}\n",
    "    d = pd.DataFrame.from_dict(d)\n",
    "    return d\n",
    "\n",
    "def calculating_metrics(X_train, X_val, X_oot, y_train, y_val, y_oot):\n",
    "    test = create_measures(y_train,model.predict_proba(X_train)[:, 1])\n",
    "    val = create_measures(y_val,model.predict_proba(X_val)[:, 1])\n",
    "    oot = create_measures(y_oot,model.predict_proba(X_oot)[:, 1]) \n",
    "\n",
    "    measures =  pd.concat([test,val,oot]).set_index([pd.Index(['TRAIN', 'VAL', 'OOT'])]) \n",
    "    \n",
    "    return measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Budowa modelu XGB na domyślnych parametrach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC</th>\n",
       "      <th>GINI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TRAIN</th>\n",
       "      <td>0.8618</td>\n",
       "      <td>0.7236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VAL</th>\n",
       "      <td>0.8271</td>\n",
       "      <td>0.6542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OOT</th>\n",
       "      <td>0.8330</td>\n",
       "      <td>0.6659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          AUC    GINI\n",
       "TRAIN  0.8618  0.7236\n",
       "VAL    0.8271  0.6542\n",
       "OOT    0.8330  0.6659"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xgb.XGBClassifier(tree_method='gpu_hist')\n",
    "model.fit(X_train, y_train)  \n",
    "measures = calculating_metrics(X_train, X_test, X_val, y_train, y_test, y_val)\n",
    "measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T21:15:16.848427Z",
     "start_time": "2019-07-16T21:15:16.842425Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T21:23:52.257902Z",
     "start_time": "2019-07-16T21:23:52.245895Z"
    }
   },
   "outputs": [],
   "source": [
    "grid_param = {  \n",
    "    'n_estimators': [50, 100, 200, 500],\n",
    "    'max_depth': [4, 5, 6, 7],\n",
    "    'learning_rate': [0.05, 0.1, 0.25, 0.5],\n",
    "    'subsample': [0.75, 1.00], \n",
    "    'tree_method': ['gpu_hist']    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T21:23:53.488755Z",
     "start_time": "2019-07-16T21:23:53.481748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "print(4*4*4*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T21:23:53.932058Z",
     "start_time": "2019-07-16T21:23:53.922052Z"
    }
   },
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=model,  \n",
    "                     param_grid=grid_param,\n",
    "                     scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T21:29:08.957026Z",
     "start_time": "2019-07-16T21:23:56.510752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 500, 'subsample': 0.75, 'tree_method': 'gpu_hist'}\n"
     ]
    }
   ],
   "source": [
    "grid_search.fit(X_train, y_train) \n",
    "\n",
    "best_parameters = grid_search.best_params_  \n",
    "print(best_parameters) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC</th>\n",
       "      <th>GINI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TRAIN</th>\n",
       "      <td>0.9625</td>\n",
       "      <td>0.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VAL</th>\n",
       "      <td>0.8872</td>\n",
       "      <td>0.7743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OOT</th>\n",
       "      <td>0.8893</td>\n",
       "      <td>0.7787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          AUC    GINI\n",
       "TRAIN  0.9625  0.9250\n",
       "VAL    0.8872  0.7743\n",
       "OOT    0.8893  0.7787"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xgb.XGBClassifier(**best_parameters)\n",
    "model.fit(X_train, y_train)  \n",
    "measures = calculating_metrics(X_train, X_test, X_val, y_train, y_test, y_val)\n",
    "measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_subsample</th>\n",
       "      <th>param_tree_method</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.927680</td>\n",
       "      <td>0.006464</td>\n",
       "      <td>0.081022</td>\n",
       "      <td>0.000463</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>gpu_hist</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 4, 'n_est...</td>\n",
       "      <td>0.766104</td>\n",
       "      <td>0.765055</td>\n",
       "      <td>0.760859</td>\n",
       "      <td>0.764006</td>\n",
       "      <td>0.002266</td>\n",
       "      <td>127</td>\n",
       "      <td>0.799312</td>\n",
       "      <td>0.798138</td>\n",
       "      <td>0.800407</td>\n",
       "      <td>0.799286</td>\n",
       "      <td>0.000927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.981021</td>\n",
       "      <td>0.068542</td>\n",
       "      <td>0.081362</td>\n",
       "      <td>0.002029</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>gpu_hist</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 4, 'n_est...</td>\n",
       "      <td>0.759030</td>\n",
       "      <td>0.763020</td>\n",
       "      <td>0.755335</td>\n",
       "      <td>0.759128</td>\n",
       "      <td>0.003138</td>\n",
       "      <td>128</td>\n",
       "      <td>0.793934</td>\n",
       "      <td>0.797439</td>\n",
       "      <td>0.799289</td>\n",
       "      <td>0.796887</td>\n",
       "      <td>0.002221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.210635</td>\n",
       "      <td>0.013439</td>\n",
       "      <td>0.106923</td>\n",
       "      <td>0.000367</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>0.75</td>\n",
       "      <td>gpu_hist</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 4, 'n_est...</td>\n",
       "      <td>0.809919</td>\n",
       "      <td>0.813059</td>\n",
       "      <td>0.804162</td>\n",
       "      <td>0.809046</td>\n",
       "      <td>0.003684</td>\n",
       "      <td>116</td>\n",
       "      <td>0.855552</td>\n",
       "      <td>0.860388</td>\n",
       "      <td>0.859106</td>\n",
       "      <td>0.858349</td>\n",
       "      <td>0.002046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.232666</td>\n",
       "      <td>0.008130</td>\n",
       "      <td>0.107586</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>gpu_hist</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 4, 'n_est...</td>\n",
       "      <td>0.807670</td>\n",
       "      <td>0.809825</td>\n",
       "      <td>0.799418</td>\n",
       "      <td>0.805638</td>\n",
       "      <td>0.004485</td>\n",
       "      <td>117</td>\n",
       "      <td>0.855859</td>\n",
       "      <td>0.860550</td>\n",
       "      <td>0.858918</td>\n",
       "      <td>0.858443</td>\n",
       "      <td>0.001944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.115632</td>\n",
       "      <td>0.051965</td>\n",
       "      <td>0.161368</td>\n",
       "      <td>0.001163</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>0.75</td>\n",
       "      <td>gpu_hist</td>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 4, 'n_est...</td>\n",
       "      <td>0.847199</td>\n",
       "      <td>0.849941</td>\n",
       "      <td>0.842179</td>\n",
       "      <td>0.846440</td>\n",
       "      <td>0.003214</td>\n",
       "      <td>69</td>\n",
       "      <td>0.906116</td>\n",
       "      <td>0.909011</td>\n",
       "      <td>0.910266</td>\n",
       "      <td>0.908464</td>\n",
       "      <td>0.001738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.927680      0.006464         0.081022        0.000463   \n",
       "1       0.981021      0.068542         0.081362        0.002029   \n",
       "2       1.210635      0.013439         0.106923        0.000367   \n",
       "3       1.232666      0.008130         0.107586        0.000270   \n",
       "4       2.115632      0.051965         0.161368        0.001163   \n",
       "\n",
       "  param_learning_rate param_max_depth param_n_estimators param_subsample  \\\n",
       "0                0.05               4                 50            0.75   \n",
       "1                0.05               4                 50               1   \n",
       "2                0.05               4                100            0.75   \n",
       "3                0.05               4                100               1   \n",
       "4                0.05               4                200            0.75   \n",
       "\n",
       "  param_tree_method                                             params  \\\n",
       "0          gpu_hist  {'learning_rate': 0.05, 'max_depth': 4, 'n_est...   \n",
       "1          gpu_hist  {'learning_rate': 0.05, 'max_depth': 4, 'n_est...   \n",
       "2          gpu_hist  {'learning_rate': 0.05, 'max_depth': 4, 'n_est...   \n",
       "3          gpu_hist  {'learning_rate': 0.05, 'max_depth': 4, 'n_est...   \n",
       "4          gpu_hist  {'learning_rate': 0.05, 'max_depth': 4, 'n_est...   \n",
       "\n",
       "   split0_test_score  split1_test_score  split2_test_score  mean_test_score  \\\n",
       "0           0.766104           0.765055           0.760859         0.764006   \n",
       "1           0.759030           0.763020           0.755335         0.759128   \n",
       "2           0.809919           0.813059           0.804162         0.809046   \n",
       "3           0.807670           0.809825           0.799418         0.805638   \n",
       "4           0.847199           0.849941           0.842179         0.846440   \n",
       "\n",
       "   std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
       "0        0.002266              127            0.799312            0.798138   \n",
       "1        0.003138              128            0.793934            0.797439   \n",
       "2        0.003684              116            0.855552            0.860388   \n",
       "3        0.004485              117            0.855859            0.860550   \n",
       "4        0.003214               69            0.906116            0.909011   \n",
       "\n",
       "   split2_train_score  mean_train_score  std_train_score  \n",
       "0            0.800407          0.799286         0.000927  \n",
       "1            0.799289          0.796887         0.002221  \n",
       "2            0.859106          0.858349         0.002046  \n",
       "3            0.858918          0.858443         0.001944  \n",
       "4            0.910266          0.908464         0.001738  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GS_results = pd.DataFrame(grid_search.cv_results_)\n",
    "GS_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(style=\"white\", rc={'figure.figsize':(12,5), 'axes.grid': False}, font_scale=0)\n",
    "ax = sns.barplot(x=GS_results.index, y='mean_test_score', data=GS_results, color=\"gray\")\n",
    "\n",
    "ax.set_title(\"Przykład: Grid Search\",fontsize=16,weight='bold')\n",
    "ax.set_xlabel(\"Kolejna iteracja w przeszukiwaniu parametrów\",fontsize=12)\n",
    "ax.set_ylabel(\"AUC\", fontsize=12)\n",
    "ax.set(ylim=(0.7, 1.0));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T21:14:03.281936Z",
     "start_time": "2019-07-16T21:14:03.276933Z"
    }
   },
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_param = {\n",
    "    'max_depth': range(1,9),\n",
    "    'learning_rate': np.logspace(np.log10(0.005), np.log10(0.5), base = 10, num = 1000),\n",
    "    'n_estimators': range(100, 1000, 50),\n",
    "    'gamma': np.linspace(0, 1, 10),\n",
    "    'min_child_weight': range(1,100,5),\n",
    "    'subsample': np.linspace(0.5, 1, 101),\n",
    "    'colsample_bytree': np.linspace(0.5, 1, 101),\n",
    "    'colsample_bylevel': np.linspace(0.5, 1, 101),\n",
    "    'reg_alpha': np.linspace(0, 2),\n",
    "    'reg_lambda': np.linspace(0, 2),\n",
    "    'tree_method': ['gpu_hist']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier()\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=model\n",
    "                                 , param_distributions=random_param\n",
    "                                 , n_iter=4*4*4*2\n",
    "                                 , scoring='roc_auc' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tree_method': 'gpu_hist', 'subsample': 0.51, 'reg_lambda': 1.8367346938775508, 'reg_alpha': 1.3061224489795917, 'n_estimators': 800, 'min_child_weight': 91, 'max_depth': 5, 'learning_rate': 0.062239357309395296, 'gamma': 0.4444444444444444, 'colsample_bytree': 0.75, 'colsample_bylevel': 0.95}\n"
     ]
    }
   ],
   "source": [
    "random_search.fit(X=X_train, y=y_train)\n",
    "\n",
    "best_parameters = random_search.best_params_  \n",
    "print(best_parameters) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC</th>\n",
       "      <th>GINI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TRAIN</th>\n",
       "      <td>0.9433</td>\n",
       "      <td>0.8866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VAL</th>\n",
       "      <td>0.8922</td>\n",
       "      <td>0.7845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OOT</th>\n",
       "      <td>0.8959</td>\n",
       "      <td>0.7919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          AUC    GINI\n",
       "TRAIN  0.9433  0.8866\n",
       "VAL    0.8922  0.7845\n",
       "OOT    0.8959  0.7919"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xgb.XGBClassifier(**best_parameters)\n",
    "model.fit(X_train, y_train)  \n",
    "measures = calculating_metrics(X_train, X_test, X_val, y_train, y_test, y_val)\n",
    "measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_tree_method</th>\n",
       "      <th>param_subsample</th>\n",
       "      <th>param_reg_lambda</th>\n",
       "      <th>param_reg_alpha</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_min_child_weight</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_gamma</th>\n",
       "      <th>param_colsample_bytree</th>\n",
       "      <th>param_colsample_bylevel</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.127529</td>\n",
       "      <td>0.055261</td>\n",
       "      <td>0.287415</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>gpu_hist</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1.7551</td>\n",
       "      <td>0.44898</td>\n",
       "      <td>650</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0434419</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.595</td>\n",
       "      <td>{'tree_method': 'gpu_hist', 'subsample': 0.66,...</td>\n",
       "      <td>0.879137</td>\n",
       "      <td>0.878527</td>\n",
       "      <td>0.873437</td>\n",
       "      <td>0.877034</td>\n",
       "      <td>0.002556</td>\n",
       "      <td>40</td>\n",
       "      <td>0.921499</td>\n",
       "      <td>0.923363</td>\n",
       "      <td>0.924562</td>\n",
       "      <td>0.923142</td>\n",
       "      <td>0.001260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.227743</td>\n",
       "      <td>0.019453</td>\n",
       "      <td>0.779975</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>gpu_hist</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.653061</td>\n",
       "      <td>0.163265</td>\n",
       "      <td>950</td>\n",
       "      <td>76</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00680933</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.51</td>\n",
       "      <td>{'tree_method': 'gpu_hist', 'subsample': 0.67,...</td>\n",
       "      <td>0.853031</td>\n",
       "      <td>0.852901</td>\n",
       "      <td>0.845120</td>\n",
       "      <td>0.850351</td>\n",
       "      <td>0.003699</td>\n",
       "      <td>85</td>\n",
       "      <td>0.897515</td>\n",
       "      <td>0.899134</td>\n",
       "      <td>0.899658</td>\n",
       "      <td>0.898769</td>\n",
       "      <td>0.000912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.629249</td>\n",
       "      <td>0.002117</td>\n",
       "      <td>0.211346</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>gpu_hist</td>\n",
       "      <td>0.595</td>\n",
       "      <td>1.18367</td>\n",
       "      <td>0.897959</td>\n",
       "      <td>650</td>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>0.041869</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.705</td>\n",
       "      <td>0.705</td>\n",
       "      <td>{'tree_method': 'gpu_hist', 'subsample': 0.595...</td>\n",
       "      <td>0.866803</td>\n",
       "      <td>0.867269</td>\n",
       "      <td>0.859781</td>\n",
       "      <td>0.864618</td>\n",
       "      <td>0.003425</td>\n",
       "      <td>62</td>\n",
       "      <td>0.889146</td>\n",
       "      <td>0.890763</td>\n",
       "      <td>0.891818</td>\n",
       "      <td>0.890575</td>\n",
       "      <td>0.001099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.769842</td>\n",
       "      <td>0.012597</td>\n",
       "      <td>0.236970</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>gpu_hist</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.204082</td>\n",
       "      <td>1.59184</td>\n",
       "      <td>250</td>\n",
       "      <td>96</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0249844</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.92</td>\n",
       "      <td>{'tree_method': 'gpu_hist', 'subsample': 0.52,...</td>\n",
       "      <td>0.848582</td>\n",
       "      <td>0.848299</td>\n",
       "      <td>0.839424</td>\n",
       "      <td>0.845435</td>\n",
       "      <td>0.004252</td>\n",
       "      <td>90</td>\n",
       "      <td>0.880316</td>\n",
       "      <td>0.881830</td>\n",
       "      <td>0.883205</td>\n",
       "      <td>0.881784</td>\n",
       "      <td>0.001180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.506671</td>\n",
       "      <td>0.012588</td>\n",
       "      <td>0.450489</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>gpu_hist</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.367347</td>\n",
       "      <td>0.938776</td>\n",
       "      <td>700</td>\n",
       "      <td>91</td>\n",
       "      <td>5</td>\n",
       "      <td>0.083213</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.715</td>\n",
       "      <td>{'tree_method': 'gpu_hist', 'subsample': 0.9, ...</td>\n",
       "      <td>0.892096</td>\n",
       "      <td>0.890095</td>\n",
       "      <td>0.886136</td>\n",
       "      <td>0.889442</td>\n",
       "      <td>0.002477</td>\n",
       "      <td>5</td>\n",
       "      <td>0.968031</td>\n",
       "      <td>0.969799</td>\n",
       "      <td>0.969300</td>\n",
       "      <td>0.969043</td>\n",
       "      <td>0.000744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       2.127529      0.055261         0.287415        0.000633   \n",
       "1       6.227743      0.019453         0.779975        0.002174   \n",
       "2       1.629249      0.002117         0.211346        0.000257   \n",
       "3       1.769842      0.012597         0.236970        0.000480   \n",
       "4       3.506671      0.012588         0.450489        0.001020   \n",
       "\n",
       "  param_tree_method param_subsample param_reg_lambda param_reg_alpha  \\\n",
       "0          gpu_hist            0.66           1.7551         0.44898   \n",
       "1          gpu_hist            0.67         0.653061        0.163265   \n",
       "2          gpu_hist           0.595          1.18367        0.897959   \n",
       "3          gpu_hist            0.52         0.204082         1.59184   \n",
       "4          gpu_hist             0.9         0.367347        0.938776   \n",
       "\n",
       "  param_n_estimators param_min_child_weight param_max_depth  \\\n",
       "0                650                     11               3   \n",
       "1                950                     76               7   \n",
       "2                650                     36               2   \n",
       "3                250                     96               7   \n",
       "4                700                     91               5   \n",
       "\n",
       "  param_learning_rate param_gamma param_colsample_bytree  \\\n",
       "0           0.0434419    0.444444                  0.765   \n",
       "1          0.00680933    0.444444                   0.87   \n",
       "2            0.041869    0.444444                  0.705   \n",
       "3           0.0249844    0.888889                  0.655   \n",
       "4            0.083213    0.666667                  0.875   \n",
       "\n",
       "  param_colsample_bylevel                                             params  \\\n",
       "0                   0.595  {'tree_method': 'gpu_hist', 'subsample': 0.66,...   \n",
       "1                    0.51  {'tree_method': 'gpu_hist', 'subsample': 0.67,...   \n",
       "2                   0.705  {'tree_method': 'gpu_hist', 'subsample': 0.595...   \n",
       "3                    0.92  {'tree_method': 'gpu_hist', 'subsample': 0.52,...   \n",
       "4                   0.715  {'tree_method': 'gpu_hist', 'subsample': 0.9, ...   \n",
       "\n",
       "   split0_test_score  split1_test_score  split2_test_score  mean_test_score  \\\n",
       "0           0.879137           0.878527           0.873437         0.877034   \n",
       "1           0.853031           0.852901           0.845120         0.850351   \n",
       "2           0.866803           0.867269           0.859781         0.864618   \n",
       "3           0.848582           0.848299           0.839424         0.845435   \n",
       "4           0.892096           0.890095           0.886136         0.889442   \n",
       "\n",
       "   std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
       "0        0.002556               40            0.921499            0.923363   \n",
       "1        0.003699               85            0.897515            0.899134   \n",
       "2        0.003425               62            0.889146            0.890763   \n",
       "3        0.004252               90            0.880316            0.881830   \n",
       "4        0.002477                5            0.968031            0.969799   \n",
       "\n",
       "   split2_train_score  mean_train_score  std_train_score  \n",
       "0            0.924562          0.923142         0.001260  \n",
       "1            0.899658          0.898769         0.000912  \n",
       "2            0.891818          0.890575         0.001099  \n",
       "3            0.883205          0.881784         0.001180  \n",
       "4            0.969300          0.969043         0.000744  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RS_results = pd.DataFrame(random_search.cv_results_)\n",
    "RS_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(style=\"white\", rc={'figure.figsize':(12,5), 'axes.grid': False}, font_scale=0)\n",
    "ax = sns.barplot(x=RS_results.index, y='mean_test_score', data=RS_results, color=\"gray\")\n",
    "\n",
    "ax.set_title(\"Przykład: Random Search\",fontsize=16,weight='bold')\n",
    "ax.set_xlabel(\"Kolejna iteracja w przeszukiwaniu parametrów\",fontsize=12)\n",
    "ax.set_ylabel(\"AUC\", fontsize=12)\n",
    "ax.set(ylim=(0.7, 1.0));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T21:29:09.004061Z",
     "start_time": "2019-07-16T21:28:28.415Z"
    }
   },
   "source": [
    "### Baysian Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T21:15:50.690953Z",
     "start_time": "2019-07-17T21:15:50.681962Z"
    }
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials, partial # procedury do optymalizacji hiperparametrów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-17T21:28:14.628514Z",
     "start_time": "2019-07-17T21:28:14.620514Z"
    }
   },
   "outputs": [],
   "source": [
    "# parametr mówiący ile iteracji najpierw losujemy zanim zaczniemy optymalizować parametry\n",
    "n_startup_jobs = 4*4*2*1 # liczba całkowita > 0\n",
    "\n",
    "# parametr ile łącznie robimy iteracji \n",
    "max_evals = 4*4*4*2 # liczba całkowita > n_startup_jobs\n",
    "\n",
    "BS_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Określenie zakresu do przeszukiwania dla hiperparametrów.\n",
    "# Można modyfikować dowolnie według uznania\n",
    "space ={\n",
    "    'learning_rate': hp.uniform ('x_learning_rate', 0.01, 0.5),\n",
    "    'max_depth': hp.quniform ('x_max_depth', 1, 9, 1),\n",
    "    'n_estimators': hp.quniform ('x_n_estimators', 100, 1000, 50),\n",
    "    'min_child_weight': hp.quniform ('x_min_child_weight', 0, 100, 1),\n",
    "    'gamma': hp.loguniform ('x_gamma', 0.0, 2.0),\n",
    "    'subsample': hp.uniform ('x_subsample', 0.5, 1.0),    \n",
    "    'colsample_bytree': hp.uniform ('x_colsample_bytree', 0.5, 1.0),\n",
    "    'colsample_bylevel': hp.uniform ('x_colsample_bylevel', 0.5, 1.0),\n",
    "    'reg_alpha': hp.loguniform ('x_reg_alpha', 0.0, 2.0),\n",
    "    'reg_lambda': hp.loguniform ('x_reg_lambda', 0.0, 2.0),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T05:18:37.164329Z",
     "start_time": "2019-07-18T05:18:37.142319Z"
    }
   },
   "outputs": [],
   "source": [
    "def objective(space):\n",
    "    xgb_params = {\n",
    "        # ogólne\n",
    "        'learning_rate': space['learning_rate'],\n",
    "        'max_depth': int(space['max_depth']),\n",
    "        'n_estimators': int(space['n_estimators']),\n",
    "        'min_child_weight': int(space['min_child_weight']),\n",
    "        'gamma': space['gamma'],\n",
    "        'seed': 2019,\n",
    "        # do walki z overfiting\n",
    "        'subsample': space['subsample'],\n",
    "        'colsample_bytree': space['colsample_bytree'],\n",
    "        'colsample_bylevel': space['colsample_bylevel'],\n",
    "        # regularyzacja\n",
    "        'reg_alpha': space['reg_alpha'],\n",
    "        'reg_lambda': space['reg_lambda'],\n",
    "        # pozostałe\n",
    "        'metric': 'auc',  \n",
    "        'eval_metric': 'auc', \n",
    "        'tree_method': 'gpu_hist'\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBClassifier(**xgb_params)\n",
    "    %time model.fit(X_train, y_train)\n",
    "    y_pred = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    score = - roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    BS_results.append([roc_auc_score(y_test, y_pred), xgb_params])\n",
    "    \n",
    "    print(\"SCORE: {0}\".format(roc_auc_score(y_test, y_pred)))\n",
    "    \n",
    "    return{'loss':score, 'status': STATUS_OK }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "UWAGA! Hyperopt szuka minimum. Zatem jak Twoja metryka im wyższa tym lepiej (jak AUC) wówczas należy przemnożyć ją przez -1 jak w powyższym przykładzie:  score = - roc_auc_score(y_test, y_pred).<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-18T05:19:25.957070Z",
     "start_time": "2019-07-18T05:18:38.541498Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.87 s, sys: 687 ms, total: 2.56 s\n",
      "Wall time: 2.57 s\n",
      "SCORE: 0.8680561826646037\n",
      "CPU times: user 4.72 s, sys: 1.5 s, total: 6.22 s\n",
      "Wall time: 6.22 s\n",
      "SCORE: 0.8775939016973284\n",
      "CPU times: user 1.53 s, sys: 567 ms, total: 2.1 s\n",
      "Wall time: 2.09 s\n",
      "SCORE: 0.8899357160674447\n",
      "CPU times: user 2.37 s, sys: 806 ms, total: 3.18 s\n",
      "Wall time: 3.18 s\n",
      "SCORE: 0.8572744789739796\n",
      "CPU times: user 5.44 s, sys: 1.81 s, total: 7.25 s\n",
      "Wall time: 7.25 s\n",
      "SCORE: 0.8901453631767173\n",
      "CPU times: user 3.63 s, sys: 1.43 s, total: 5.06 s\n",
      "Wall time: 5.06 s\n",
      "SCORE: 0.8838704750053251\n",
      "CPU times: user 9.3 s, sys: 2.98 s, total: 12.3 s\n",
      "Wall time: 12.3 s\n",
      "SCORE: 0.8814238108050539\n",
      "CPU times: user 3.84 s, sys: 839 ms, total: 4.68 s\n",
      "Wall time: 4.68 s\n",
      "SCORE: 0.8692700660881849\n",
      "CPU times: user 743 ms, sys: 252 ms, total: 995 ms\n",
      "Wall time: 994 ms\n",
      "SCORE: 0.8569442587641118\n",
      "CPU times: user 3.08 s, sys: 1.05 s, total: 4.13 s\n",
      "Wall time: 4.12 s\n",
      "SCORE: 0.8568534750950123\n",
      "CPU times: user 7.26 s, sys: 2.38 s, total: 9.64 s\n",
      "Wall time: 9.63 s\n",
      "SCORE: 0.8698810734985819\n",
      "CPU times: user 1.4 s, sys: 451 ms, total: 1.85 s\n",
      "Wall time: 1.85 s\n",
      "SCORE: 0.8709276449287549\n",
      "CPU times: user 1.23 s, sys: 350 ms, total: 1.58 s\n",
      "Wall time: 1.58 s\n",
      "SCORE: 0.8681566847722508\n",
      "CPU times: user 2.18 s, sys: 711 ms, total: 2.89 s\n",
      "Wall time: 2.89 s\n",
      "SCORE: 0.8425242820827589\n",
      "CPU times: user 1.8 s, sys: 614 ms, total: 2.41 s\n",
      "Wall time: 2.41 s\n",
      "SCORE: 0.8594925251404164\n",
      "CPU times: user 1.25 s, sys: 483 ms, total: 1.73 s\n",
      "Wall time: 1.73 s\n",
      "SCORE: 0.8851448727003665\n",
      "CPU times: user 4.02 s, sys: 1.28 s, total: 5.3 s\n",
      "Wall time: 5.29 s\n",
      "SCORE: 0.8684810613067412\n",
      "CPU times: user 5.34 s, sys: 1.76 s, total: 7.1 s\n",
      "Wall time: 7.09 s\n",
      "SCORE: 0.8783030289016693\n",
      "CPU times: user 1.87 s, sys: 637 ms, total: 2.51 s\n",
      "Wall time: 2.51 s\n",
      "SCORE: 0.8878658498133387\n",
      "CPU times: user 6.4 s, sys: 2.04 s, total: 8.44 s\n",
      "Wall time: 8.44 s\n",
      "SCORE: 0.8540674853417638\n",
      "CPU times: user 5.04 s, sys: 1.84 s, total: 6.88 s\n",
      "Wall time: 6.88 s\n",
      "SCORE: 0.8743712793865401\n",
      "CPU times: user 827 ms, sys: 308 ms, total: 1.14 s\n",
      "Wall time: 1.13 s\n",
      "SCORE: 0.8779804320956514\n",
      "CPU times: user 5.69 s, sys: 1.7 s, total: 7.39 s\n",
      "Wall time: 7.39 s\n",
      "SCORE: 0.8647170336831129\n",
      "CPU times: user 1.53 s, sys: 626 ms, total: 2.16 s\n",
      "Wall time: 2.15 s\n",
      "SCORE: 0.894655710125674\n",
      "CPU times: user 1.93 s, sys: 625 ms, total: 2.56 s\n",
      "Wall time: 2.55 s\n",
      "SCORE: 0.8750969426507023\n",
      "CPU times: user 1.37 s, sys: 555 ms, total: 1.93 s\n",
      "Wall time: 1.93 s\n",
      "SCORE: 0.866590047253893\n",
      "CPU times: user 5.75 s, sys: 1.93 s, total: 7.68 s\n",
      "Wall time: 7.67 s\n",
      "SCORE: 0.8821348578739673\n",
      "CPU times: user 1.69 s, sys: 609 ms, total: 2.3 s\n",
      "Wall time: 2.29 s\n",
      "SCORE: 0.8777428348411976\n",
      "CPU times: user 2.53 s, sys: 749 ms, total: 3.28 s\n",
      "Wall time: 3.28 s\n",
      "SCORE: 0.880856259599323\n",
      "CPU times: user 5.75 s, sys: 1.86 s, total: 7.61 s\n",
      "Wall time: 7.6 s\n",
      "SCORE: 0.8862488263601609\n",
      "CPU times: user 7.53 s, sys: 2.3 s, total: 9.84 s\n",
      "Wall time: 9.83 s\n",
      "SCORE: 0.8590565547259498\n",
      "CPU times: user 4.41 s, sys: 1.41 s, total: 5.82 s\n",
      "Wall time: 5.82 s\n",
      "SCORE: 0.873183128454355\n",
      "CPU times: user 6.3 s, sys: 1.95 s, total: 8.25 s\n",
      "Wall time: 8.24 s\n",
      "SCORE: 0.8769788163264163\n",
      "CPU times: user 2.73 s, sys: 990 ms, total: 3.72 s\n",
      "Wall time: 3.71 s\n",
      "SCORE: 0.8833659219834304\n",
      "CPU times: user 1.58 s, sys: 662 ms, total: 2.24 s\n",
      "Wall time: 2.24 s\n",
      "SCORE: 0.8566364533234678\n",
      "CPU times: user 5.86 s, sys: 2.05 s, total: 7.91 s\n",
      "Wall time: 7.91 s\n",
      "SCORE: 0.8620268024305204\n",
      "CPU times: user 3.92 s, sys: 1.22 s, total: 5.13 s\n",
      "Wall time: 5.13 s\n",
      "SCORE: 0.8856034680882072\n",
      "CPU times: user 4.93 s, sys: 1.79 s, total: 6.72 s\n",
      "Wall time: 6.71 s\n",
      "SCORE: 0.8631285244229195\n",
      "CPU times: user 1.62 s, sys: 737 ms, total: 2.36 s\n",
      "Wall time: 2.35 s\n",
      "SCORE: 0.7972555080494176\n",
      "CPU times: user 5.2 s, sys: 1.79 s, total: 7 s\n",
      "Wall time: 6.99 s\n",
      "SCORE: 0.8467829516025965\n",
      "CPU times: user 2.14 s, sys: 845 ms, total: 2.99 s\n",
      "Wall time: 2.99 s\n",
      "SCORE: 0.8799784785984148\n",
      "CPU times: user 1.84 s, sys: 644 ms, total: 2.49 s\n",
      "Wall time: 2.49 s\n",
      "SCORE: 0.8855361572158881\n",
      "CPU times: user 5.1 s, sys: 1.72 s, total: 6.82 s\n",
      "Wall time: 6.82 s\n",
      "SCORE: 0.8759038918597741\n",
      "CPU times: user 6.51 s, sys: 2.12 s, total: 8.63 s\n",
      "Wall time: 8.62 s\n",
      "SCORE: 0.8917782325194228\n",
      "CPU times: user 1.39 s, sys: 496 ms, total: 1.88 s\n",
      "Wall time: 1.88 s\n",
      "SCORE: 0.8806917047836859\n",
      "CPU times: user 1.28 s, sys: 555 ms, total: 1.84 s\n",
      "Wall time: 1.84 s\n",
      "SCORE: 0.8934768046726981\n",
      "CPU times: user 1.19 s, sys: 427 ms, total: 1.62 s\n",
      "Wall time: 1.62 s\n",
      "SCORE: 0.892146758932275\n",
      "CPU times: user 1.75 s, sys: 696 ms, total: 2.45 s\n",
      "Wall time: 2.44 s\n",
      "SCORE: 0.8862220008071839\n",
      "CPU times: user 999 ms, sys: 402 ms, total: 1.4 s\n",
      "Wall time: 1.4 s\n",
      "SCORE: 0.889459309381271\n",
      "CPU times: user 1.99 s, sys: 775 ms, total: 2.76 s\n",
      "Wall time: 2.76 s\n",
      "SCORE: 0.8719618248242693\n",
      "CPU times: user 1.9 s, sys: 815 ms, total: 2.71 s\n",
      "Wall time: 2.71 s\n",
      "SCORE: 0.8841351640713462\n",
      "CPU times: user 1.18 s, sys: 430 ms, total: 1.61 s\n",
      "Wall time: 1.61 s\n",
      "SCORE: 0.8898408229072077\n",
      "CPU times: user 1.46 s, sys: 519 ms, total: 1.98 s\n",
      "Wall time: 1.97 s\n",
      "SCORE: 0.8804770793394544\n",
      "CPU times: user 1.65 s, sys: 649 ms, total: 2.3 s\n",
      "Wall time: 2.3 s\n",
      "SCORE: 0.886022065129654\n",
      "CPU times: user 2.13 s, sys: 838 ms, total: 2.97 s\n",
      "Wall time: 2.97 s\n",
      "SCORE: 0.8826440283803629\n",
      "CPU times: user 601 ms, sys: 189 ms, total: 790 ms\n",
      "Wall time: 788 ms\n",
      "SCORE: 0.8632292612585344\n",
      "CPU times: user 1.53 s, sys: 569 ms, total: 2.1 s\n",
      "Wall time: 2.1 s\n",
      "SCORE: 0.8930227251706857\n",
      "CPU times: user 1.96 s, sys: 731 ms, total: 2.69 s\n",
      "Wall time: 2.69 s\n",
      "SCORE: 0.8849242038868149\n",
      "CPU times: user 1.39 s, sys: 554 ms, total: 1.95 s\n",
      "Wall time: 1.94 s\n",
      "SCORE: 0.894290031979058\n",
      "CPU times: user 2.97 s, sys: 1.03 s, total: 4 s\n",
      "Wall time: 3.99 s\n",
      "SCORE: 0.8734007948519603\n",
      "CPU times: user 1.85 s, sys: 731 ms, total: 2.59 s\n",
      "Wall time: 2.58 s\n",
      "SCORE: 0.8838551301023555\n",
      "CPU times: user 7.26 s, sys: 2.44 s, total: 9.7 s\n",
      "Wall time: 9.7 s\n",
      "SCORE: 0.8694135199105371\n",
      "CPU times: user 4.73 s, sys: 1.7 s, total: 6.44 s\n",
      "Wall time: 6.43 s\n",
      "SCORE: 0.8729141477202659\n",
      "CPU times: user 2.53 s, sys: 1.05 s, total: 3.58 s\n",
      "Wall time: 3.57 s\n",
      "SCORE: 0.8777417417796163\n",
      "CPU times: user 1.47 s, sys: 556 ms, total: 2.03 s\n",
      "Wall time: 2.03 s\n",
      "SCORE: 0.890525636498167\n",
      "CPU times: user 1.57 s, sys: 607 ms, total: 2.17 s\n",
      "Wall time: 2.17 s\n",
      "SCORE: 0.8935808241964596\n",
      "CPU times: user 1.52 s, sys: 653 ms, total: 2.17 s\n",
      "Wall time: 2.17 s\n",
      "SCORE: 0.8924978103734347\n",
      "CPU times: user 1.6 s, sys: 687 ms, total: 2.28 s\n",
      "Wall time: 2.28 s\n",
      "SCORE: 0.8941287423345553\n",
      "CPU times: user 2.16 s, sys: 872 ms, total: 3.03 s\n",
      "Wall time: 3.03 s\n",
      "SCORE: 0.8848569350553257\n",
      "CPU times: user 1.8 s, sys: 702 ms, total: 2.5 s\n",
      "Wall time: 2.5 s\n",
      "SCORE: 0.8940974219161649\n",
      "CPU times: user 2.04 s, sys: 840 ms, total: 2.88 s\n",
      "Wall time: 2.88 s\n",
      "SCORE: 0.8878374547360398\n",
      "CPU times: user 1.41 s, sys: 628 ms, total: 2.04 s\n",
      "Wall time: 2.04 s\n",
      "SCORE: 0.8933161946882813\n",
      "CPU times: user 2.26 s, sys: 879 ms, total: 3.13 s\n",
      "Wall time: 3.13 s\n",
      "SCORE: 0.8791885734425274\n",
      "CPU times: user 1.66 s, sys: 725 ms, total: 2.39 s\n",
      "Wall time: 2.39 s\n",
      "SCORE: 0.8936647306864427\n",
      "CPU times: user 2.08 s, sys: 804 ms, total: 2.88 s\n",
      "Wall time: 2.88 s\n",
      "SCORE: 0.8837946788921399\n",
      "CPU times: user 2.69 s, sys: 934 ms, total: 3.63 s\n",
      "Wall time: 3.62 s\n",
      "SCORE: 0.8637834294666981\n",
      "CPU times: user 2.34 s, sys: 954 ms, total: 3.3 s\n",
      "Wall time: 3.29 s\n",
      "SCORE: 0.8690328682216167\n",
      "CPU times: user 1.71 s, sys: 716 ms, total: 2.43 s\n",
      "Wall time: 2.43 s\n",
      "SCORE: 0.8892359149205709\n",
      "CPU times: user 1.35 s, sys: 496 ms, total: 1.84 s\n",
      "Wall time: 1.84 s\n",
      "SCORE: 0.892953354297694\n",
      "CPU times: user 1.52 s, sys: 633 ms, total: 2.15 s\n",
      "Wall time: 2.15 s\n",
      "SCORE: 0.8932184847924304\n",
      "CPU times: user 1.72 s, sys: 625 ms, total: 2.35 s\n",
      "Wall time: 2.35 s\n",
      "SCORE: 0.884951106514647\n",
      "CPU times: user 4.65 s, sys: 1.68 s, total: 6.33 s\n",
      "Wall time: 6.33 s\n",
      "SCORE: 0.8690708275877531\n",
      "CPU times: user 930 ms, sys: 357 ms, total: 1.29 s\n",
      "Wall time: 1.29 s\n",
      "SCORE: 0.8838948551833541\n",
      "CPU times: user 3.41 s, sys: 1.3 s, total: 4.71 s\n",
      "Wall time: 4.71 s\n",
      "SCORE: 0.8750541906299398\n",
      "CPU times: user 1.37 s, sys: 578 ms, total: 1.95 s\n",
      "Wall time: 1.94 s\n",
      "SCORE: 0.8922640248209059\n",
      "CPU times: user 646 ms, sys: 231 ms, total: 877 ms\n",
      "Wall time: 875 ms\n",
      "SCORE: 0.8787872166448054\n",
      "CPU times: user 1.02 s, sys: 368 ms, total: 1.39 s\n",
      "Wall time: 1.39 s\n",
      "SCORE: 0.8688300387336181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.13 s, sys: 1.99 s, total: 8.12 s\n",
      "Wall time: 8.11 s\n",
      "SCORE: 0.8719119854202402\n",
      "CPU times: user 2.48 s, sys: 1.01 s, total: 3.48 s\n",
      "Wall time: 3.48 s\n",
      "SCORE: 0.8801047202042624\n",
      "CPU times: user 1.12 s, sys: 501 ms, total: 1.62 s\n",
      "Wall time: 1.62 s\n",
      "SCORE: 0.8917319911097659\n",
      "CPU times: user 1.86 s, sys: 754 ms, total: 2.62 s\n",
      "Wall time: 2.62 s\n",
      "SCORE: 0.8830765794739852\n",
      "CPU times: user 3.52 s, sys: 1.23 s, total: 4.74 s\n",
      "Wall time: 4.74 s\n",
      "SCORE: 0.875063253932219\n",
      "CPU times: user 1.17 s, sys: 421 ms, total: 1.59 s\n",
      "Wall time: 1.59 s\n",
      "SCORE: 0.8875341126301864\n",
      "CPU times: user 2.53 s, sys: 1.04 s, total: 3.56 s\n",
      "Wall time: 3.56 s\n",
      "SCORE: 0.8695116957589211\n",
      "CPU times: user 1.22 s, sys: 509 ms, total: 1.73 s\n",
      "Wall time: 1.72 s\n",
      "SCORE: 0.8933236254049933\n",
      "CPU times: user 3.64 s, sys: 1.33 s, total: 4.97 s\n",
      "Wall time: 4.97 s\n",
      "SCORE: 0.8627481249789797\n",
      "CPU times: user 1.96 s, sys: 775 ms, total: 2.74 s\n",
      "Wall time: 2.74 s\n",
      "SCORE: 0.8800831917958721\n",
      "CPU times: user 1.43 s, sys: 628 ms, total: 2.06 s\n",
      "Wall time: 2.06 s\n",
      "SCORE: 0.8935216867621835\n",
      "CPU times: user 2.26 s, sys: 835 ms, total: 3.1 s\n",
      "Wall time: 3.09 s\n",
      "SCORE: 0.8816687967353894\n",
      "CPU times: user 6.94 s, sys: 2.25 s, total: 9.19 s\n",
      "Wall time: 9.18 s\n",
      "SCORE: 0.8683622223903853\n",
      "CPU times: user 1.06 s, sys: 449 ms, total: 1.51 s\n",
      "Wall time: 1.51 s\n",
      "SCORE: 0.8766177731813138\n",
      "CPU times: user 2.17 s, sys: 946 ms, total: 3.11 s\n",
      "Wall time: 3.11 s\n",
      "SCORE: 0.8818658806432808\n",
      "CPU times: user 878 ms, sys: 305 ms, total: 1.18 s\n",
      "Wall time: 1.18 s\n",
      "SCORE: 0.8867513088711757\n",
      "CPU times: user 2.13 s, sys: 763 ms, total: 2.9 s\n",
      "Wall time: 2.89 s\n",
      "SCORE: 0.8620973399365464\n",
      "CPU times: user 2.02 s, sys: 850 ms, total: 2.87 s\n",
      "Wall time: 2.87 s\n",
      "SCORE: 0.8881133336696598\n",
      "CPU times: user 1.16 s, sys: 372 ms, total: 1.54 s\n",
      "Wall time: 1.53 s\n",
      "SCORE: 0.8816561529557506\n",
      "CPU times: user 1 s, sys: 397 ms, total: 1.4 s\n",
      "Wall time: 1.4 s\n",
      "SCORE: 0.8890525958811197\n",
      "CPU times: user 3.32 s, sys: 919 ms, total: 4.24 s\n",
      "Wall time: 4.24 s\n",
      "SCORE: 0.8696861336730233\n",
      "CPU times: user 3.86 s, sys: 1.44 s, total: 5.29 s\n",
      "Wall time: 5.29 s\n",
      "SCORE: 0.862772648796511\n",
      "CPU times: user 5.58 s, sys: 1.97 s, total: 7.54 s\n",
      "Wall time: 7.54 s\n",
      "SCORE: 0.8644058124250272\n",
      "CPU times: user 1.92 s, sys: 814 ms, total: 2.74 s\n",
      "Wall time: 2.73 s\n",
      "SCORE: 0.8867867072500812\n",
      "CPU times: user 1.37 s, sys: 581 ms, total: 1.95 s\n",
      "Wall time: 1.95 s\n",
      "SCORE: 0.8885831224285026\n",
      "CPU times: user 1.85 s, sys: 729 ms, total: 2.58 s\n",
      "Wall time: 2.58 s\n",
      "SCORE: 0.8878115821085436\n",
      "CPU times: user 5.75 s, sys: 1.96 s, total: 7.71 s\n",
      "Wall time: 7.71 s\n",
      "SCORE: 0.8651062126537292\n",
      "CPU times: user 1.62 s, sys: 643 ms, total: 2.27 s\n",
      "Wall time: 2.26 s\n",
      "SCORE: 0.890059116413861\n",
      "CPU times: user 1.23 s, sys: 493 ms, total: 1.73 s\n",
      "Wall time: 1.73 s\n",
      "SCORE: 0.8920290305945134\n",
      "CPU times: user 2.78 s, sys: 1.11 s, total: 3.89 s\n",
      "Wall time: 3.89 s\n",
      "SCORE: 0.8682328977903339\n",
      "CPU times: user 6.92 s, sys: 2.38 s, total: 9.3 s\n",
      "Wall time: 9.3 s\n",
      "SCORE: 0.8532324003071784\n",
      "CPU times: user 1.99 s, sys: 763 ms, total: 2.76 s\n",
      "Wall time: 2.75 s\n",
      "SCORE: 0.8838496823114609\n",
      "CPU times: user 6.41 s, sys: 2.21 s, total: 8.62 s\n",
      "Wall time: 8.61 s\n",
      "SCORE: 0.8699943805423828\n",
      "CPU times: user 1.37 s, sys: 586 ms, total: 1.95 s\n",
      "Wall time: 1.95 s\n",
      "SCORE: 0.8945133528683057\n",
      "CPU times: user 912 ms, sys: 371 ms, total: 1.28 s\n",
      "Wall time: 1.28 s\n",
      "SCORE: 0.8887000204598706\n",
      "CPU times: user 1.33 s, sys: 513 ms, total: 1.84 s\n",
      "Wall time: 1.84 s\n",
      "SCORE: 0.8889859331382638\n",
      "CPU times: user 1.8 s, sys: 746 ms, total: 2.54 s\n",
      "Wall time: 2.54 s\n",
      "SCORE: 0.87604803585242\n",
      "CPU times: user 1.39 s, sys: 547 ms, total: 1.94 s\n",
      "Wall time: 1.93 s\n",
      "SCORE: 0.8610048318927344\n",
      "CPU times: user 3.7 s, sys: 1.33 s, total: 5.03 s\n",
      "Wall time: 5.02 s\n",
      "SCORE: 0.8462827042623796\n",
      "CPU times: user 1.61 s, sys: 584 ms, total: 2.2 s\n",
      "Wall time: 2.19 s\n",
      "SCORE: 0.8865410311494523\n",
      "CPU times: user 1.54 s, sys: 617 ms, total: 2.16 s\n",
      "Wall time: 2.15 s\n",
      "SCORE: 0.8856943253287592\n",
      "The best params:  {'x_colsample_bylevel': 0.8623877485439386, 'x_colsample_bytree': 0.7389197245332271, 'x_gamma': 3.4430670991133705, 'x_learning_rate': 0.43792732671175894, 'x_max_depth': 1.0, 'x_min_child_weight': 90.0, 'x_n_estimators': 850.0, 'x_reg_alpha': 4.069597437502552, 'x_reg_lambda': 4.260805469567687, 'x_subsample': 0.7722362118142974}\n"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "best_params = fmin(fn=objective,\n",
    "                   space=space,\n",
    "                   algo=partial(tpe.suggest, n_startup_jobs=n_startup_jobs),\n",
    "                   max_evals=max_evals,\n",
    "                   trials=trials)\n",
    "\n",
    "print(\"The best params: \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.868056</td>\n",
       "      <td>{'learning_rate': 0.03808810753340928, 'max_de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.877594</td>\n",
       "      <td>{'learning_rate': 0.19582031905145278, 'max_de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.889936</td>\n",
       "      <td>{'learning_rate': 0.1528042623883467, 'max_dep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.857274</td>\n",
       "      <td>{'learning_rate': 0.4739350270885256, 'max_dep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.890145</td>\n",
       "      <td>{'learning_rate': 0.09616115814543878, 'max_de...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      score                                             params\n",
       "0  0.868056  {'learning_rate': 0.03808810753340928, 'max_de...\n",
       "1  0.877594  {'learning_rate': 0.19582031905145278, 'max_de...\n",
       "2  0.889936  {'learning_rate': 0.1528042623883467, 'max_dep...\n",
       "3  0.857274  {'learning_rate': 0.4739350270885256, 'max_dep...\n",
       "4  0.890145  {'learning_rate': 0.09616115814543878, 'max_de..."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BS_results = pd.DataFrame(BS_results)\n",
    "BS_results.columns = ['score','params']\n",
    "BS_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set(style=\"white\", rc={'figure.figsize':(12,5), 'axes.grid': False}, font_scale=0)\n",
    "ax = sns.barplot(x=BS_results.index, y='score', data=BS_results, color=\"gray\")\n",
    "\n",
    "ax.set_title(\"Przykład: Baysian Search\",fontsize=16,weight='bold')\n",
    "ax.set_xlabel(\"Kolejna iteracja w przeszukiwaniu parametrów\",fontsize=12)\n",
    "ax.set_ylabel(\"AUC\", fontsize=12)\n",
    "ax.set(ylim=(0.7, 1.0));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_colsample_bylevel': 0.8623877485439386,\n",
       " 'x_colsample_bytree': 0.7389197245332271,\n",
       " 'x_gamma': 3.4430670991133705,\n",
       " 'x_learning_rate': 0.43792732671175894,\n",
       " 'x_max_depth': 1.0,\n",
       " 'x_min_child_weight': 90.0,\n",
       " 'x_n_estimators': 850.0,\n",
       " 'x_reg_alpha': 4.069597437502552,\n",
       " 'x_reg_lambda': 4.260805469567687,\n",
       " 'x_subsample': 0.7722362118142974}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC</th>\n",
       "      <th>GINI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TRAIN</th>\n",
       "      <td>0.9120</td>\n",
       "      <td>0.8241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VAL</th>\n",
       "      <td>0.8934</td>\n",
       "      <td>0.7869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OOT</th>\n",
       "      <td>0.8995</td>\n",
       "      <td>0.7989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          AUC    GINI\n",
       "TRAIN  0.9120  0.8241\n",
       "VAL    0.8934  0.7869\n",
       "OOT    0.8995  0.7989"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xgb.XGBClassifier(**{\n",
    " 'colsample_bylevel': 0.8623877485439386,\n",
    " 'colsample_bytree': 0.7389197245332271,\n",
    " 'gamma': 3.4430670991133705,\n",
    " 'learning_rate': 0.43792732671175894,\n",
    " 'max_depth': 1,\n",
    " 'min_child_weight': 90.0,\n",
    " 'n_estimators': 850,\n",
    " 'reg_alpha': 4.069597437502552,\n",
    " 'reg_lambda': 4.260805469567687,\n",
    " 'subsample': 0.7722362118142974,\n",
    " 'tree_method': 'gpu_hist'\n",
    "})\n",
    "model.fit(X_train, y_train)  \n",
    "measures = calculating_metrics(X_train, X_test, X_val, y_train, y_test, y_val)\n",
    "measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "GS_results.to_csv('GS_results.csv')\n",
    "RS_results.to_csv('RS_results.csv')\n",
    "BS_results.to_csv('BS_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
